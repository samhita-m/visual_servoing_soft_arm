{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "os.chdir('/home/a-m/marri2/visual_servoing/attempt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b5mcsngV9IjX"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import models\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils, transforms\n",
    "import re\n",
    "from torch.nn import ReplicationPad3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_v7DV_lPf4bq"
   },
   "outputs": [],
   "source": [
    "# global variable\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cJak_Tjwf4br"
   },
   "outputs": [],
   "source": [
    "class ImagePoseDataset(Dataset):\n",
    "    \n",
    "    #reading csv file with labels\n",
    "    def __init__(self, image_dir, size = 600, transform=None):\n",
    "        annotation_files = glob.glob(image_dir + '/annotated_*.csv')\n",
    "        \n",
    "        assert len(annotation_files) == 1, \"None or more than one annotation file found at \" + image_dir\n",
    "        self.annotations = pd.read_csv(annotation_files[0])\n",
    "        \n",
    "        self.annotations.astype({'B': 'float32','R': 'float32'}).dtypes  #change this\n",
    "        \n",
    "        self.data = glob.glob(image_dir + '/**/*.jpg', recursive=True)\n",
    "        self.data.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    \n",
    "        \n",
    "        unequal_error = \"Number of images not equal to number of annotations: {} != {}\".format(len(self.data), len(self.annotations[\"image\"].values))\n",
    "        assert len(self.data) == len(self.annotations[\"image\"].values), unequal_error\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.image_size = [480, 640] #change this\n",
    "        self.transform = transform\n",
    "        self.size = size\n",
    "\n",
    "        self.pairs = []\n",
    "        \n",
    "        for i in range(len(self.data)):\n",
    "            for j in np.random.choice(range(len(self.data)), size = self.size, replace=False):\n",
    "                self.pairs.append((i,j))\n",
    "\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        np.random.seed(230)\n",
    "        \n",
    "        first_img_path = self.data[self.pairs[idx][0]]\n",
    "        second_img_path = self.data[self.pairs[idx][1]]\n",
    "        \n",
    "        img_paths = [first_img_path, second_img_path]\n",
    "        img_name = [os.path.split(img_paths[0])[-1][:-4], os.path.split(img_paths[1])[-1][:-4]]  #might have to change this\n",
    "        images = [Image.open(first_img_path), Image.open(second_img_path)]\n",
    "        \n",
    "        \n",
    "        pose = np.zeros((1, 2), dtype=np.float32)  #change this\n",
    "        annotation1 = self.annotations.loc[self.annotations[\"image\"] == img_name[0]].iloc[0]\n",
    "        annotation2 = self.annotations.loc[self.annotations[\"image\"] == img_name[1]].iloc[0]\n",
    "    \n",
    "        ann1 = annotation1[2:].to_numpy()\n",
    "        ann2 = annotation2[2:].to_numpy()\n",
    "    \n",
    "        pose[0] = ann1 - ann2\n",
    "      #  pose[1] = ann2 - ann1\n",
    "        \n",
    "       \n",
    "                \n",
    "        for i in range(len(images)):            \n",
    "            if i == 0:\n",
    "                j = torch.unsqueeze(self.transform(images[i]), dim=0)\n",
    "            else:\n",
    "                k = torch.unsqueeze(self.transform(images[i]), dim=0)\n",
    "                j = torch.cat((j,k), dim=0)\n",
    "        data = j\n",
    "        \n",
    " \n",
    "        sample = {\"images\": data , \"poses\": pose, \"filename\":img_name}        \n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "def fetch_dataloader(types, data_dir, batch_size, num_workers):\n",
    "    \"\"\"\n",
    "    Fetches the DataLoader object for each type in types from data_dir.\n",
    "\n",
    "    Args:\n",
    "        types: (list) has one or more of 'train', 'val', 'test' depending on which data is required\n",
    "        data_dir: (string) directory containing the dataset\n",
    "        params: (Params) hyperparameters\n",
    "\n",
    "    Returns:\n",
    "        data: (dict) contains the DataLoader object for each type in types\n",
    "    \"\"\"\n",
    "    dataloaders = {}\n",
    "    \n",
    "    train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) \n",
    "    \n",
    "    test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) \n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        if split in types:\n",
    "            path = os.path.join(data_dir, split)\n",
    "\n",
    "            if split == 'train':\n",
    "                dl = DataLoader(ImagePoseDataset(path, size = 100, transform = train_transforms), \n",
    "                                        batch_size=batch_size, shuffle=False,\n",
    "                                        num_workers=num_workers)\n",
    "\n",
    "            else:\n",
    "                dl = DataLoader(ImagePoseDataset(path, size = 10, transform = test_transform), \n",
    "                                batch_size=batch_size, shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "\n",
    "            dataloaders[split] = dl\n",
    "\n",
    "    return dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PsombGgzf4bs",
    "outputId": "5221339b-ce45-4b04-b887-ccabdc848017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class PoseModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PoseModel, self).__init__()\n",
    "        model1 = models.resnet50(pretrained=True)\n",
    "        model2 = models.resnet50(pretrained=True)\n",
    "        \n",
    "        self.enc1 = nn.Sequential(*list(model1.children())[:-2])\n",
    "        self.enc2 = nn.Sequential(*list(model2.children())[:-2])       \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4096, 2048, kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "        self.bn1 = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(2048, 1024, kernel_size=(3,3), stride=(2,2), padding=(1,1))  \n",
    "        self.bn2 = nn.BatchNorm2d(1024)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(1024, 512, kernel_size=(3,3), stride=(2,2), padding=(1,1))  \n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(512, 256, kernel_size=(3,3), stride=(2,2), padding=(1,1))  \n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.out= nn.Linear(128, 2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, s): \n",
    "        \n",
    "        feature1 = self.enc1(s[:,0])\n",
    "        feature2 = self.enc2(s[:,1])\n",
    "        feature = torch.cat([feature1,feature2],1)\n",
    "        x = F.relu(self.bn1(self.conv1(feature)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = torch.reshape(x, (x.size(0),-1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        x = x.view(s.size(0), 1, 2) \n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "net = PoseModel()\n",
    "\n",
    "\n",
    "dummy_input = torch.rand((2, 2, 3, 480, 640))\n",
    "output = net(dummy_input)\n",
    "\n",
    "# print(net)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pc7TPnvWf4bw"
   },
   "outputs": [],
   "source": [
    "class MyCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCriterion, self).__init__()        \n",
    "\n",
    "    def forward(self, prediction, target): \n",
    "            \n",
    "        return F.mse_loss(prediction, target, reduction=\"mean\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_rqob63Qf4bx"
   },
   "outputs": [],
   "source": [
    "def simple_train(model, criterion, optimizer, train_dataloader, **kwargs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        img, gt = batch['images'], batch['poses']     \n",
    "        img = img.to(device)\n",
    "        \n",
    "     #   print(img.shape)\n",
    "        gt = gt.to(device)\n",
    "        \n",
    "        img = img.float()\n",
    "        pred = model(img)\n",
    "        \n",
    "        loss = criterion.forward(pred,gt)\n",
    "      \n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    return sum(losses)/len(losses)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def simple_predict(dataloader, model):\n",
    "    model.eval()    \n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            \n",
    "            img, gt = batch['images'], batch['poses']  \n",
    "            img = img.to(device)\n",
    "            gt = gt.to(device)\n",
    "            img = img.float()\n",
    "            \n",
    "            pred = model(img)\n",
    "            # print(pred)\n",
    "\n",
    "            pred = pred.view(-1,2)\n",
    "            gt =  gt.view(-1,2)\n",
    "            \n",
    "            loss = torch.mean(F.mse_loss(pred, gt, reduction=\"none\"),0)\n",
    "            \n",
    "            if i==0:\n",
    "                losses = torch.clone(torch.unsqueeze(loss,0))\n",
    "                \n",
    "            losses = torch.cat([losses,torch.unsqueeze(loss,0)],0)\n",
    "    \n",
    "    losses = torch.mean(losses, 0).cpu().numpy()\n",
    "    b, r = losses[0],losses[1]\n",
    "    \n",
    "    \n",
    "\n",
    "    return np.mean(losses),losses[0],losses[1], pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "3hBj-J-Nf4bx",
    "outputId": "0208d36f-52c8-4463-a8a5-bcfc03a8018d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [52:54<00:00,  2.20it/s]\n",
      "  0%|          | 1/800 [00:00<01:33,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 29.71258188106758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:33<00:00, 23.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss : 215.32307\n",
      "Validation metrics: b error 23.5917, r error 407.0544 \n",
      "predictions: tensor([[ 5.2989, -6.9404]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [52:54<00:00,  2.20it/s] \n",
      "  0%|          | 3/800 [00:00<00:29, 27.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 15.7058200406347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:31<00:00, 25.41it/s]\n",
      "  0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss : 224.06686\n",
      "Validation metrics: b error 21.8906, r error 426.2431 \n",
      "predictions: tensor([[ 4.4226, 11.4243]], device='cuda:0')\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [52:46<00:00,  2.21it/s] \n",
      "  0%|          | 3/800 [00:00<00:37, 21.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 14.076413976337228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:31<00:00, 25.58it/s]\n",
      "  0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss : 306.72824\n",
      "Validation metrics: b error 36.8193, r error 576.6371 \n",
      "predictions: tensor([[ 5.5852, -3.5345]], device='cuda:0')\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [53:00<00:00,  2.20it/s]\n",
      "  0%|          | 2/800 [00:00<00:48, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 12.360014580702144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:33<00:00, 23.69it/s]\n",
      "  0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss : 1164.1082\n",
      "Validation metrics: b error 356.6080, r error 1971.6083 \n",
      "predictions: tensor([[33.4051, 90.3882]], device='cuda:0')\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [53:13<00:00,  2.19it/s] \n",
      "  0%|          | 2/800 [00:00<00:48, 16.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 11.219461361689227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:32<00:00, 24.90it/s]\n",
      "  0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss : 259.6426\n",
      "Validation metrics: b error 96.8095, r error 422.4757 \n",
      "predictions: tensor([[15.9155, -2.6590]], device='cuda:0')\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [53:17<00:00,  2.19it/s] \n",
      "  0%|          | 3/800 [00:00<00:29, 26.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 11.961966284751892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:31<00:00, 25.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss : 197.83582\n",
      "Validation metrics: b error 23.5435, r error 372.1281 \n",
      "predictions: tensor([[  6.3478, -11.5187]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 4798/7000 [36:59<16:58,  2.16it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-087e12c146d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-119bdd15aa72>\u001b[0m in \u001b[0;36msimple_train\u001b[0;34m(model, criterion, optimizer, train_dataloader, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'poses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/software/Python/3.7.2-IGB-gcc-8.2.0/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/software/PyTorch/1.7.0-IGB-gcc-8.2.0-Python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/software/PyTorch/1.7.0-IGB-gcc-8.2.0-Python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/software/PyTorch/1.7.0-IGB-gcc-8.2.0-Python-3.7.2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/software/PyTorch/1.7.0-IGB-gcc-8.2.0-Python-3.7.2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c7c3cbdfa803>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/software/PyTorch/1.7.0-IGB-gcc-8.2.0-Python-3.7.2/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/software/PyTorch/1.7.0-IGB-gcc-8.2.0-Python-3.7.2/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/software/PyTorch/1.7.0-IGB-gcc-8.2.0-Python-3.7.2/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/software/Python/3.7.2-IGB-gcc-8.2.0/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 50\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "model = PoseModel().to(device)\n",
    "\n",
    "\n",
    "criterion = MyCriterion().to(device)\n",
    "parameter = model.parameters()\n",
    "optimizer = optim.SGD(parameter, lr=1e-4, weight_decay=1e-2)\n",
    "\n",
    "# #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[4], gamma=0.1)\n",
    "data_dir = 'data'\n",
    "train_dataloader = fetch_dataloader(['train'], data_dir, batch_size=batch_size,num_workers=0)['train']\n",
    "val_dataloader = fetch_dataloader(['val'], data_dir, batch_size=1,num_workers=0)['val']\n",
    "test_dataloader = fetch_dataloader(['test'], data_dir, batch_size=1,num_workers=0)['test']\n",
    "print(len(train_dataloader))\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "b1 = []\n",
    "r1 = []\n",
    "theta1 = []\n",
    "x1 = []\n",
    "y1 = []\n",
    "\n",
    "predicts = []\n",
    "\n",
    "best_loss = 5000\n",
    "for epoch in range(num_epochs):\n",
    "  \n",
    "    \n",
    "    print(\"Epoch {}/{}\".format(epoch+1, num_epochs))\n",
    "\n",
    "    epoch_loss = simple_train(model, criterion, optimizer, train_dataloader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    print(\"Training loss {}\".format(epoch_loss))\n",
    "    # print(\"Training metrics: b error %.4f, r error %.4f, theta error %.4f, x error %.4f, y error %.4f\" % (train_losses[0],train_losses[1],train_losses[2], train_losses[3], train_losses[4]))\n",
    "\n",
    "    val_loss, b, r, pred = simple_predict(val_dataloader, model)\n",
    "    b1.append(b)\n",
    "    r1.append(r)\n",
    "    \n",
    "    predicts.append(pred)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(\"Validation loss :\", val_loss)\n",
    "    print(\"Validation metrics: b error %.4f, r error %.4f \" % (b, r))\n",
    "\n",
    "    print(\"predictions:\", pred)\n",
    "\n",
    "   \n",
    "    if val_loss<best_loss:          \n",
    "        torch.save(model.state_dict(), 'best_model_2_outputs.torch')         \n",
    "        best_loss = val_loss\n",
    "        \n",
    "    #scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iIt8iehf4bx"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYEMKAeuf4by"
   },
   "outputs": [],
   "source": [
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9B4nrt7IaT7A"
   },
   "outputs": [],
   "source": [
    "print(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRlgY2neiAE8"
   },
   "outputs": [],
   "source": [
    "# metrics = predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tR1uUOR8f4by"
   },
   "outputs": [],
   "source": [
    "print([[b1[i],r1[i],theta1[i],x1[i],y1[i]] for i in range(len(b1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAaOu8vBkHpZ"
   },
   "outputs": [],
   "source": [
    "# metrics = np.asarray(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyyLOBhOjEO1"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig, ax1 = plt.subplots()\n",
    "# color = 'tab:red'\n",
    "# ax1.set_xlabel('Epochs')\n",
    "# ax1.set_ylabel('Error (cm)', color=color)\n",
    "# ax1.plot(epochs, metrics[:,0], 'r')\n",
    "# ax1.plot(epochs, metrics[:,1], 'b')\n",
    "# ax1.plot(epochs, metrics[:,2], 'g')\n",
    "# ax1.tick_params(axis='y', labelcolor=color)\n",
    "# ax1.set_ylim(0, 0.1)\n",
    "# ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "# color = 'tab:blue'\n",
    "# ax2.set_ylabel('Error (radians)', color=color)  # we already handled the x-label with ax1\n",
    "# ax2.plot(epochs, metrics[:,3], 'y')\n",
    "# ax2.plot(epochs, metrics[:,4], 'c')\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\n",
    "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZC80mXDgkiKW"
   },
   "outputs": [],
   "source": [
    "# predicts1 = [[71.3455, 299.98846, 612.14197, 17.913647, 67.760956], [70.61943, 308.3565, 614.71674, 18.341103, 66.79955], [72.254776, 308.16354, 624.0446, 17.517784, 68.64179], [79.20256, 313.67682, 756.6699, 21.187294, 76.29698], [68.64581, 445.8373, 1308.4067, 45.08603, 147.08012], [67.010864, 394.3578, 1163.5549, 37.64315, 111.245186], [72.79684, 407.45255, 1438.2988, 37.703526, 122.95048], [66.26516, 509.4921, 2232.0396, 53.581795, 160.51176], [71.617165, 553.7531, 2251.1345, 50.06454, 163.87398], [67.6314, 645.1201, 3004.0298, 64.6395, 201.75027], [68.610435, 670.9459, 3495.046, 70.110756, 229.66035], [78.080925, 838.47186, 3762.1555, 78.345604, 224.2315], [82.31957, 791.64526, 3300.3245, 68.491196, 200.6706], [85.77867, 724.0587, 2986.078, 61.731506, 186.24583], [88.41423, 767.7903, 2660.4978, 58.266937, 180.84425], [92.60425, 729.06586, 2681.3442, 55.219467, 174.54987], [88.064064, 702.8218, 2273.686, 47.222137, 142.69472], [92.22376, 915.272, 3502.126, 67.31765, 197.83511], [95.04986, 732.945, 2337.5552, 47.52399, 143.4624], [103.41726, 750.2705, 1805.5945, 39.52808, 141.29509], [115.16615, 639.1609, 1685.0542, 31.669968, 111.18192], [104.19611, 698.6576, 1724.9683, 36.858562, 113.58083], [112.40713, 964.80774, 2499.877, 51.554733, 155.13557], [95.69072, 626.71136, 1799.6161, 37.875294, 117.63456], [128.13573, 694.4404, 2200.0706, 35.31282, 113.16661], [115.73999, 649.15173, 1381.557, 29.185846, 106.21257], [132.67511, 638.09875, 1506.5328, 31.595715, 90.02608], [140.15654, 648.0216, 1620.7126, 32.83686, 100.44681], [133.61388, 673.5364, 1540.6757, 31.223743, 103.85487], [120.989136, 620.9135, 2077.5657, 36.32834, 108.56132], [132.84982, 589.97534, 1586.4774, 31.332727, 85.892555], [134.00136, 630.5227, 2079.9744, 37.17443, 105.96474], [142.30118, 713.86334, 1812.0747, 34.262226, 93.07108], [131.9562, 618.09344, 1308.4608, 30.623444, 92.09023], [150.78465, 771.7664, 1536.8103, 38.427544, 88.707756], [124.79615, 650.9295, 1969.3282, 37.62735, 109.81103], [117.48657, 528.9835, 1442.0688, 30.423353, 84.737495], [166.30316, 554.52606, 1387.5977, 27.794542, 72.16256], [132.65923, 610.9276, 1272.4724, 30.719393, 83.64812], [170.80846, 651.6129, 1587.1293, 31.822659, 86.05595], [148.26839, 546.7099, 1423.1058, 23.101112, 69.18983], [119.36049, 562.3412, 1525.4735, 33.434387, 90.121826], [126.641014, 579.34705, 1643.3766, 42.385773, 106.71667], [153.49957, 608.18365, 1292.7495, 30.102694, 72.05002], [140.59573, 538.986, 1275.6699, 26.956669, 72.718094], [128.63644, 565.2077, 1283.3916, 31.26406, 80.66515], [142.91138, 589.8891, 1425.4674, 36.772514, 108.61162], [174.99702, 604.99945, 1231.4559, 25.457443, 73.68763], [124.46342, 532.1797, 1284.4299, 37.867233, 90.98839], [156.39485, 572.2624, 1304.4873, 29.487886, 64.113655], [141.00058, 599.98944, 1427.2222, 32.966682, 78.49583], [140.62863, 510.04877, 1460.5568, 28.990866, 70.09744], [122.61561, 541.0749, 1227.1143, 30.144491, 77.38699], [118.877815, 520.6939, 1433.2882, 35.451847, 67.93618], [149.10368, 488.75287, 1207.9657, 27.233482, 74.68739], [114.91516, 574.0989, 1348.9976, 40.59728, 87.81351], [123.55287, 516.64185, 1131.8646, 32.938347, 82.748245], [153.83464, 508.87488, 1146.1227, 26.891317, 54.73151], [140.56522, 608.0974, 1416.4187, 40.215973, 79.94072], [152.53659, 592.6191, 1324.523, 39.891792, 78.46605]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiRofoIskVXO"
   },
   "outputs": [],
   "source": [
    "# predicts1 = np.asarray(predicts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JXZQDtIkQY6"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# epochs = range(1,61)\n",
    "# fig, ax1 = plt.subplots()\n",
    "\n",
    "# color = 'tab:red'\n",
    "# ax1.set_xlabel('Epochs')\n",
    "# ax1.set_ylabel('Error (cm)', color=color)\n",
    "# ax1.plot(epochs, predicts1[:,0], 'r',)\n",
    "# ax1.plot(epochs, predicts1[:,1], 'b')\n",
    "# ax1.plot(epochs, predicts1[:,2], 'g')\n",
    "# ax1.tick_params(axis='y', labelcolor=color)\n",
    "# # ax1.set_ylim(0, 0.1)\n",
    "\n",
    "# ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "# color = 'tab:blue'\n",
    "# ax2.set_ylabel('Error (radians)', color=color)  # we already handled the x-label with ax1\n",
    "# ax2.plot(epochs, predicts1[:,3], 'm')\n",
    "# ax2.plot(epochs, predicts1[:,4], 'c')\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\n",
    "# plt.title('x,y,z,phi,theta,psi error')\n",
    "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXW5C3oHjGov"
   },
   "outputs": [],
   "source": [
    "# epochs = range(1,41)\n",
    "# plt.plot(epochs, train_losses, 'g', label='Training loss')\n",
    "# plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "# plt.title('Training and Validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hptcLlnf4b3"
   },
   "outputs": [],
   "source": [
    "# metrics = [[27.24967, 313.78604, 103.22942, 0.12602033, 2.4108167], [27.305887, 312.19617, 103.47133, 0.14717528, 2.3978324], [27.524355, 311.05536, 104.642555, 0.15902913, 2.4331577], [27.548677, 325.57285, 106.00729, 0.13429697, 2.4158552], [27.098442, 328.60037, 104.91552, 0.12587117, 2.472294], [27.359726, 334.21838, 104.05903, 0.12709449, 2.4215572], [27.339266, 358.40887, 106.68851, 0.11997138, 2.393511], [27.588818, 383.73975, 107.74374, 0.11348984, 2.3892288], [27.389458, 371.63693, 105.884705, 0.12097337, 2.403273], [27.360971, 342.81848, 106.109245, 0.11562351, 2.3738363], [27.719664, 393.87692, 110.986465, 0.119660616, 2.3882978], [27.633759, 339.6035, 105.70057, 0.113286234, 2.3775778], [28.07819, 360.8138, 112.83532, 0.11849985, 2.3681376], [28.364185, 360.29456, 117.695984, 0.113442786, 2.388075], [29.205482, 357.27008, 125.467896, 0.11830383, 2.386264], [28.379833, 344.7701, 112.977585, 0.10953671, 2.3661098], [28.847458, 355.65115, 118.61085, 0.111811385, 2.4001744], [27.850155, 341.3711, 114.77934, 0.10915208, 2.3725796], [27.88572, 346.0621, 118.30813, 0.110895686, 2.3910563], [27.872988, 349.0645, 115.55186, 0.10912815, 2.3672535], [28.218578, 353.3147, 131.556, 0.11358704, 2.3951552], [28.392235, 339.58942, 121.12667, 0.111038156, 2.3803837], [28.25236, 329.97418, 115.96446, 0.10942464, 2.3864708], [29.406773, 366.15808, 123.42579, 0.11040557, 2.374963], [29.454271, 344.52792, 131.52682, 0.11232563, 2.3724134], [30.187252, 347.98532, 116.14241, 0.10585186, 2.3729453], [29.41112, 349.68896, 114.18263, 0.10918645, 2.3731532], [28.895391, 348.74268, 113.15022, 0.10991674, 2.3822908], [29.371452, 335.8846, 115.51087, 0.11001146, 2.3862026], [31.134686, 344.45355, 119.50881, 0.108406834, 2.375381], [30.70894, 342.97552, 120.56805, 0.11375582, 2.3945308], [29.832468, 343.45724, 109.87155, 0.10899758, 2.3766632], [29.862116, 343.1723, 113.25473, 0.10866166, 2.3900619], [30.768106, 342.69632, 112.46852, 0.10878944, 2.376799], [31.60865, 343.18985, 121.2012, 0.11096387, 2.392029], [31.704603, 339.0673, 129.9622, 0.109866515, 2.3703597], [29.185385, 334.6232, 109.926186, 0.10787476, 2.377635], [30.564453, 349.78403, 112.91635, 0.10928899, 2.3760023], [30.50954, 335.69626, 113.32439, 0.10892574, 2.3779461], [30.807068, 329.50632, 111.41365, 0.108789176, 2.380784]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xc1SmLUlbKf"
   },
   "outputs": [],
   "source": [
    "# metrics = np.asarray(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tORQA6LLlPAX"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# # import matplotlib.pyplot as plt\n",
    "\n",
    "# epochs= range(1, 41)\n",
    "# fig, ax1 = plt.subplots()\n",
    "\n",
    "# color = 'tab:red'\n",
    "# ax1.set_xlabel('Epochs')\n",
    "# ax1.set_ylabel('Error (cm)', color=color)\n",
    "# ax1.plot(epochs, metrics[:,0], 'r',)\n",
    "# ax1.plot(epochs, metrics[:,1], 'b')\n",
    "# ax1.plot(epochs, metrics[:,2], 'g')\n",
    "# ax1.tick_params(axis='y', labelcolor=color)\n",
    "# # ax1.set_ylim(0, 0.1)\n",
    "\n",
    "# ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "# color = 'tab:blue'\n",
    "# ax2.set_ylabel('Error (radians)', color=color)  # we already handled the x-label with ax1\n",
    "# ax2.plot(epochs, metrics[:,3], 'm')\n",
    "# ax2.plot(epochs, metrics[:,4], 'c')\n",
    "# # ax2.plot(epochs, metrics[:,5], 'y')\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\n",
    "# plt.title('x,y,z,phi,theta,psi error')\n",
    "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFcBZhn0lYxk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Basic model corl.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
