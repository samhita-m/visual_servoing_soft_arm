{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of absolute_actuations_exp1-5 load model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5mcsngV9IjX",
        "outputId": "8dfa6e47-27de-4bc4-f1a8-7503d1568ae8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aClCGALS9N6c",
        "outputId": "9f4805b9-640d-4531-948a-59c9f6b1cc4e"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive')\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_YleJ_xiBkd",
        "outputId": "11b0cde4-8d4d-420d-f0c2-811fae90dcd5"
      },
      "source": [
        "%tensorflow_version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently selected TF version: 2.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUAVH2STVC_U"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "from skimage.io import imread\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras import regularizers\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation, concatenate\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import random\n",
        "import math\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras import callbacks\n",
        "from keras import layers, models\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6iETxCaGyaj"
      },
      "source": [
        "IMG_WIDTH=256\n",
        "IMG_HEIGHT=256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOcam1t5G8sN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96974571-75bd-43e4-91c9-3220d76da8c6"
      },
      "source": [
        "def create_dataset(path):\n",
        "  img_data_array = []\n",
        "\n",
        "  for i in trange(1, 4126):\n",
        "            \n",
        "        path2 = \"img\"+ str(i) +\".jpg\"\n",
        "        path1 = os.path.join(path, path2)\n",
        "        # print(path1)\n",
        "        image= cv2.imread(path1, cv2.COLOR_BGR2RGB)\n",
        "        image=cv2.resize(image, (224, 224),interpolation = cv2.INTER_AREA)\n",
        "        # cv2_imshow(image)\n",
        "        image=np.array(image)\n",
        "        image = image.astype('float32')\n",
        "        image /= 255 \n",
        "        img_data_array.append(image)\n",
        "      \n",
        "        # print(image)\n",
        "\n",
        "  return img_data_array\n",
        "\n",
        "\n",
        "# extract the image array and class name\n",
        "img_data=create_dataset('10psi-images')     #check if the images are mapped correctly to labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4125/4125 [20:11<00:00,  3.40it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VguOMN0XG-2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d7fe8a-7fb1-4e69-a17d-1db20635070c"
      },
      "source": [
        "train_data = np.asarray(img_data)\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4125, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHcfWA4WIA3z"
      },
      "source": [
        "y_train = pd.read_csv('10psi-labels-all.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy57sGOsBdNh"
      },
      "source": [
        "num_classes = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVMG4LIy51aw",
        "outputId": "3afc06ba-97d0-45f8-cf03-b95ec0904b23"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4125, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3J9DPmoEonU"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(train_data, y_train, test_size = 0.3, random_state=600, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcvAQLCmlWL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4096e721-1346-447d-efcd-0f198f9463fe"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       B   R  Theta   X   Y\n",
            "2317  18  10    -20  16  18\n",
            "443   22  12    -10  16  16\n",
            "692   20  10     10  16  16\n",
            "2739  30   4     20  16  18\n",
            "2649  22  24     20  16  18\n",
            "...   ..  ..    ...  ..  ..\n",
            "410   20  -4    -10  16  16\n",
            "2506  12 -12     20  16  18\n",
            "3375  16 -24     10  18  16\n",
            "954   20 -16    -20  16  16\n",
            "3275  30 -24    -10  18  16\n",
            "\n",
            "[2887 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEYix5VfIEhe"
      },
      "source": [
        "input_shape = (224, 224, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFnAalhWA-O2"
      },
      "source": [
        "maxb = y_train.iloc[:,0].max()\n",
        "minb = y_train.iloc[:,0].min()\n",
        "\n",
        "maxr = y_train.iloc[:,1].max()       #finding the min and max of r, x, y columns in the train data\n",
        "minr = y_train.iloc[:,1].min()\n",
        "\n",
        "maxt = y_train.iloc[:,2].max()\n",
        "mint = y_train.iloc[:,2].min()\n",
        "\n",
        "maxx = y_train.iloc[:,3].max()\n",
        "minx = y_train.iloc[:,3].min()\n",
        "\n",
        "maxy = y_train.iloc[:,4].max()\n",
        "miny = y_train.iloc[:,4].min()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBHpjTPABEXE",
        "outputId": "89f20181-d68a-4cf9-8912-576c68592063"
      },
      "source": [
        "print(maxb, minb, maxr, minr, maxt, mint, maxx, minx, maxy, miny)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30 10 24 -24 20 -20 18 16 18 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "MPzbZ0xgChPG",
        "outputId": "c09fdf45-fc08-4035-b1b8-d3767b910999"
      },
      "source": [
        "y_train_scaled = y_train      #creating another variable y_train_scaled where i copy y_train\n",
        "y_train_scaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B</th>\n",
              "      <th>R</th>\n",
              "      <th>Theta</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2317</th>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>-20</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>-10</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>692</th>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2739</th>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2649</th>\n",
              "      <td>22</td>\n",
              "      <td>24</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>20</td>\n",
              "      <td>-4</td>\n",
              "      <td>-10</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2506</th>\n",
              "      <td>12</td>\n",
              "      <td>-12</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3375</th>\n",
              "      <td>16</td>\n",
              "      <td>-24</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>954</th>\n",
              "      <td>20</td>\n",
              "      <td>-16</td>\n",
              "      <td>-20</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3275</th>\n",
              "      <td>30</td>\n",
              "      <td>-24</td>\n",
              "      <td>-10</td>\n",
              "      <td>18</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2887 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       B   R  Theta   X   Y\n",
              "2317  18  10    -20  16  18\n",
              "443   22  12    -10  16  16\n",
              "692   20  10     10  16  16\n",
              "2739  30   4     20  16  18\n",
              "2649  22  24     20  16  18\n",
              "...   ..  ..    ...  ..  ..\n",
              "410   20  -4    -10  16  16\n",
              "2506  12 -12     20  16  18\n",
              "3375  16 -24     10  18  16\n",
              "954   20 -16    -20  16  16\n",
              "3275  30 -24    -10  18  16\n",
              "\n",
              "[2887 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fiTHrVbB1ZE"
      },
      "source": [
        "for i in range(len(y_train_scaled.iloc[:,0])):    #scaling \n",
        "  y_train_scaled.iloc[i,0] = (2 *(y_train_scaled.iloc[i,0] - minb)/(maxb-minb)) - 1   #\n",
        "\n",
        "for i in range(len(y_train_scaled.iloc[:,1])):    #scaling \n",
        "  y_train_scaled.iloc[i,1] = (2 *(y_train_scaled.iloc[i,1] - minr)/(maxr-minr)) - 1   #\n",
        "\n",
        "for i in range(len(y_train_scaled.iloc[:,2])):    #scaling \n",
        "  y_train_scaled.iloc[i,2] = (2 *(y_train_scaled.iloc[i,2] - mint)/(maxt-mint)) - 1   #\n",
        "\n",
        "for i in range(len(y_train_scaled.iloc[:,3])):    #scaling \n",
        "  y_train_scaled.iloc[i,3] = (2 *(y_train_scaled.iloc[i,3] - minx)/(maxx-minx)) - 1   #\n",
        "\n",
        "for i in range(len(y_train_scaled.iloc[:,4])):    #scaling \n",
        "  y_train_scaled.iloc[i,4] = (2 *(y_train_scaled.iloc[i,4] - miny)/(maxy-miny)) - 1   #\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "EK4yXzbXVTc4",
        "outputId": "bc422b53-2344-4609-86e4-d1ac97d29df9"
      },
      "source": [
        "y_train_scaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B</th>\n",
              "      <th>R</th>\n",
              "      <th>Theta</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2317</th>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>692</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2739</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2649</th>\n",
              "      <td>0.2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.166667</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2506</th>\n",
              "      <td>-0.8</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3375</th>\n",
              "      <td>-0.4</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>954</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.666667</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3275</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2887 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        B         R  Theta    X    Y\n",
              "2317 -0.2  0.416667   -1.0 -1.0  1.0\n",
              "443   0.2  0.500000   -0.5 -1.0 -1.0\n",
              "692   0.0  0.416667    0.5 -1.0 -1.0\n",
              "2739  1.0  0.166667    1.0 -1.0  1.0\n",
              "2649  0.2  1.000000    1.0 -1.0  1.0\n",
              "...   ...       ...    ...  ...  ...\n",
              "410   0.0 -0.166667   -0.5 -1.0 -1.0\n",
              "2506 -0.8 -0.500000    1.0 -1.0  1.0\n",
              "3375 -0.4 -1.000000    0.5  1.0 -1.0\n",
              "954   0.0 -0.666667   -1.0 -1.0 -1.0\n",
              "3275  1.0 -1.000000   -0.5  1.0 -1.0\n",
              "\n",
              "[2887 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD1m_MT1D3oC"
      },
      "source": [
        "maxb = y_test.iloc[:,0].max()\n",
        "minb = y_test.iloc[:,0].min()\n",
        "\n",
        "maxr = y_test.iloc[:,1].max()       #finding the min and max of r, x, y columns in the test data\n",
        "minr = y_test.iloc[:,1].min()\n",
        "\n",
        "maxt = y_test.iloc[:,2].max()\n",
        "mint = y_test.iloc[:,2].min()\n",
        "\n",
        "maxx = y_test.iloc[:,3].max()\n",
        "minx = y_test.iloc[:,3].min()\n",
        "\n",
        "maxy = y_test.iloc[:,4].max()\n",
        "miny = y_test.iloc[:,4].min()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7635z3kZEWoM",
        "outputId": "36bcd331-221f-412f-e04a-6a6de97790d3"
      },
      "source": [
        "print(maxb, minb, maxr, minr, maxt, mint, maxx, minx, maxy, miny)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30 10 24 -24 20 -20 18 16 18 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "LsLosVosECGp",
        "outputId": "3265a098-36c9-429d-d5d6-5961fea21f2d"
      },
      "source": [
        "y_test_scaled = y_test\n",
        "y_test_scaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B</th>\n",
              "      <th>R</th>\n",
              "      <th>Theta</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2893</th>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3055</th>\n",
              "      <td>12</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>18</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2155</th>\n",
              "      <td>28</td>\n",
              "      <td>-14</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>26</td>\n",
              "      <td>-12</td>\n",
              "      <td>-10</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665</th>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3874</th>\n",
              "      <td>10</td>\n",
              "      <td>24</td>\n",
              "      <td>20</td>\n",
              "      <td>18</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3552</th>\n",
              "      <td>30</td>\n",
              "      <td>-20</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619</th>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>-10</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2555</th>\n",
              "      <td>16</td>\n",
              "      <td>-14</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1238 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       B   R  Theta   X   Y\n",
              "2893  20  12      0  18  16\n",
              "3055  12 -14    -10  18  16\n",
              "2155  28 -14     10  16  18\n",
              "481   26 -12    -10  16  16\n",
              "665   18   6     10  16  16\n",
              "...   ..  ..    ...  ..  ..\n",
              "3874  10  24     20  18  16\n",
              "3552  30 -20     10  18  16\n",
              "619   14  14     10  16  16\n",
              "539   30   4    -10  16  16\n",
              "2555  16 -14     20  16  18\n",
              "\n",
              "[1238 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyV2A8bREN_Q"
      },
      "source": [
        "for i in range(len(y_test_scaled.iloc[:,0])):    #scaling \n",
        "  y_test_scaled.iloc[i,0] = (2 *(y_test_scaled.iloc[i,0] - minb)/(maxb-minb)) - 1   #\n",
        "\n",
        "for i in range(len(y_test_scaled.iloc[:,1])):    #scaling \n",
        "  y_test_scaled.iloc[i,1] = (2 *(y_test_scaled.iloc[i,1] - minr)/(maxr-minr)) - 1   #\n",
        "\n",
        "for i in range(len(y_test_scaled.iloc[:,2])):    #scaling \n",
        "  y_test_scaled.iloc[i,2] = (2 *(y_test_scaled.iloc[i,2] - mint)/(maxt-mint)) - 1   #\n",
        "\n",
        "for i in range(len(y_test_scaled.iloc[:,3])):    #scaling \n",
        "  y_test_scaled.iloc[i,3] = (2 *(y_test_scaled.iloc[i,3] - minx)/(maxx-minx)) - 1   #\n",
        "\n",
        "for i in range(len(y_test_scaled.iloc[:,4])):    #scaling \n",
        "  y_test_scaled.iloc[i,4] = (2 *(y_test_scaled.iloc[i,4] - miny)/(maxy-miny)) - 1   #\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDg0jryivygj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1942d35f-c025-4fe0-f11e-9556703f19b8"
      },
      "source": [
        "new_input = Input(shape=(224, 224, 3))\n",
        "vgg_model = VGG16(include_top=False, weights='imagenet', input_tensor=new_input)\n",
        "\n",
        "for layer in vgg_model.layers[:15]:\n",
        "  layer.trainable = False\n",
        "\n",
        "\n",
        "for layer in vgg_model.layers:\n",
        "    print(layer, layer.trainable)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x7f686e4d3bd0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f686e8af5d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f685c2e6dd0> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f685c27b810> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f686e4aec50> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f685c289090> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f686e3e5710> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f685c27ba90> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f685c24b710> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f686ef72810> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f685c3253d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f67bc4021d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f685c25acd0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f6880099b10> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f685c13f850> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f686ed9df10> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f685c154b10> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f685c15c390> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f685c3950d0> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlmvUKDGMzlZ"
      },
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(vgg_model)\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0005)))\n",
        "  model.add(layers.Dropout(0.4))\n",
        "  \n",
        "  model.add(layers.BatchNormalization())\n",
        "  \n",
        "  model.add(layers.Dense(5, activation='tanh', kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0005)))\n",
        "  model.add(layers.Dropout(0.4) )  # L1 + L2 penalties\n",
        "  model.compile(optimizer= Adam(0.00005), # adam\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mse'])\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ7t3EShuWwE"
      },
      "source": [
        "# def create_model():\n",
        "#   model = Sequential()\n",
        "#   model.add(vgg_model)\n",
        "#   model.add(layers.Flatten())\n",
        "#   model.add(layers.BatchNormalization())\n",
        "#   model.add(layers.Dense(64, activation='relu'))\n",
        "#   model.add(layers.Dropout(0.25))\n",
        "#   model.add(layers.BatchNormalization())\n",
        "#   model.add(layers.Dense(3, activation='tanh'))\n",
        "#   model.add(layers.Dropout(0.25))\n",
        "#   model.compile(optimizer= Adam(0.0003),#\"adam\",\n",
        "#               loss='mean_squared_error',\n",
        "#               metrics=['mse'])\n",
        "#   return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGh1O-V4m7Ub",
        "outputId": "a4de3e1b-3a7f-4fd6-cfb8-5bc21f34af6f"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 25088)             100352    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                1605696   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 325       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 16,421,317\n",
            "Trainable params: 8,735,749\n",
            "Non-trainable params: 7,685,568\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipbxNNNkNIo8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fc5b4ee7-373e-4fe1-a82e-2fcd8c463211"
      },
      "source": [
        "'''\n",
        "initial_learning_rate = 0.01\n",
        "epochs = 100\n",
        "decay = initial_learning_rate / epochs  \n",
        "def lr_time_based_decay(epoch, lr):\n",
        "  red_lr = 0.0001\n",
        "  if epoch < 20:  \n",
        "    lr = 0.01\n",
        "\n",
        "  else:\n",
        "    lr = red_lr * 1 / (1 + decay * epoch)\n",
        "\n",
        "  return lr\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ninitial_learning_rate = 0.01\\nepochs = 100\\ndecay = initial_learning_rate / epochs  \\ndef lr_time_based_decay(epoch, lr):\\n  red_lr = 0.0001\\n  if epoch < 20:  \\n    lr = 0.01\\n\\n  else:\\n    lr = red_lr * 1 / (1 + decay * epoch)\\n\\n  return lr\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwAk6gNgjRz5"
      },
      "source": [
        "initial_learning_rate = 0.01\n",
        "epochs = 100\n",
        "decay = initial_learning_rate / epochs\n",
        "def lr_time_based_decay(epoch, lr):\n",
        "    return lr * 1 / (1 + decay * epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LLcG_DpWKp7",
        "outputId": "5aaee6fc-1a10-42c6-8cd5-ff4d5ed6657b"
      },
      "source": [
        "history = model.fit(x_train, y_train_scaled,\n",
        "          epochs=150,\n",
        "          batch_size = 128,\n",
        "          validation_split=0.3,\n",
        "          callbacks=[callbacks.LearningRateScheduler(lr_time_based_decay, verbose=1)],\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 4.999999873689376e-05.\n",
            "16/16 [==============================] - 15s 823ms/step - loss: 2.6995 - mse: 1.3933 - val_loss: 2.1425 - val_mse: 0.8569\n",
            "Epoch 2/150\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 4.999499923697006e-05.\n",
            "16/16 [==============================] - 12s 789ms/step - loss: 2.4890 - mse: 1.2087 - val_loss: 2.0321 - val_mse: 0.7672\n",
            "Epoch 3/150\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 4.998500315338214e-05.\n",
            "16/16 [==============================] - 13s 797ms/step - loss: 2.3389 - mse: 1.0786 - val_loss: 1.9551 - val_mse: 0.7085\n",
            "Epoch 4/150\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 4.9970011984655534e-05.\n",
            "16/16 [==============================] - 13s 809ms/step - loss: 2.2023 - mse: 0.9599 - val_loss: 1.9135 - val_mse: 0.6834\n",
            "Epoch 5/150\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 4.9950030865240805e-05.\n",
            "16/16 [==============================] - 13s 815ms/step - loss: 2.1375 - mse: 0.9112 - val_loss: 1.8548 - val_mse: 0.6400\n",
            "Epoch 6/150\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 4.992506856369651e-05.\n",
            "16/16 [==============================] - 13s 821ms/step - loss: 2.0685 - mse: 0.8573 - val_loss: 1.8256 - val_mse: 0.6251\n",
            "Epoch 7/150\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 4.989513020927853e-05.\n",
            "16/16 [==============================] - 13s 826ms/step - loss: 2.0304 - mse: 0.8333 - val_loss: 1.7957 - val_mse: 0.6085\n",
            "Epoch 8/150\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 4.986022820006049e-05.\n",
            "16/16 [==============================] - 13s 840ms/step - loss: 1.9684 - mse: 0.7844 - val_loss: 1.7794 - val_mse: 0.6048\n",
            "Epoch 9/150\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 4.9820371294090025e-05.\n",
            "16/16 [==============================] - 13s 854ms/step - loss: 1.9284 - mse: 0.7567 - val_loss: 1.7628 - val_mse: 0.6000\n",
            "Epoch 10/150\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 4.977557188062224e-05.\n",
            "16/16 [==============================] - 14s 866ms/step - loss: 1.8625 - mse: 0.7024 - val_loss: 1.7487 - val_mse: 0.5969\n",
            "Epoch 11/150\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 4.972584597830597e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.8439 - mse: 0.6947 - val_loss: 1.7490 - val_mse: 0.6076\n",
            "Epoch 12/150\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 4.967120596540818e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.7987 - mse: 0.6598 - val_loss: 1.7386 - val_mse: 0.6070\n",
            "Epoch 13/150\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 4.9611671482487435e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.7813 - mse: 0.6520 - val_loss: 1.7239 - val_mse: 0.6016\n",
            "Epoch 14/150\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 4.954725852900066e-05.\n",
            "16/16 [==============================] - 14s 871ms/step - loss: 1.7517 - mse: 0.6315 - val_loss: 1.7079 - val_mse: 0.5942\n",
            "Epoch 15/150\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 4.947799036379949e-05.\n",
            "16/16 [==============================] - 14s 871ms/step - loss: 1.7361 - mse: 0.6244 - val_loss: 1.6942 - val_mse: 0.5887\n",
            "Epoch 16/150\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 4.9403882971385895e-05.\n",
            "16/16 [==============================] - 14s 869ms/step - loss: 1.7192 - mse: 0.6156 - val_loss: 1.6794 - val_mse: 0.5815\n",
            "Epoch 17/150\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 4.932496322638031e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.7168 - mse: 0.6207 - val_loss: 1.6755 - val_mse: 0.5848\n",
            "Epoch 18/150\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 4.92412543608668e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.6959 - mse: 0.6069 - val_loss: 1.6690 - val_mse: 0.5850\n",
            "Epoch 19/150\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 4.915277959765332e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.6472 - mse: 0.5648 - val_loss: 1.6650 - val_mse: 0.5874\n",
            "Epoch 20/150\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 4.905956578135605e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.6711 - mse: 0.5949 - val_loss: 1.6552 - val_mse: 0.5833\n",
            "Epoch 21/150\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 4.8961643376592136e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.6383 - mse: 0.5679 - val_loss: 1.6482 - val_mse: 0.5819\n",
            "Epoch 22/150\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 4.885903920546336e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.6303 - mse: 0.5654 - val_loss: 1.6301 - val_mse: 0.5694\n",
            "Epoch 23/150\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 4.8751783709359034e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.6193 - mse: 0.5599 - val_loss: 1.6367 - val_mse: 0.5814\n",
            "Epoch 24/150\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 4.8639910947150496e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.6037 - mse: 0.5499 - val_loss: 1.6282 - val_mse: 0.5784\n",
            "Epoch 25/150\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 4.852345496411818e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.5905 - mse: 0.5420 - val_loss: 1.6204 - val_mse: 0.5759\n",
            "Epoch 26/150\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 4.840244979195836e-05.\n",
            "16/16 [==============================] - 14s 870ms/step - loss: 1.5925 - mse: 0.5494 - val_loss: 1.6176 - val_mse: 0.5785\n",
            "Epoch 27/150\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 4.8276929448789915e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.5844 - mse: 0.5467 - val_loss: 1.6073 - val_mse: 0.5735\n",
            "Epoch 28/150\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 4.814693156734386e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.5548 - mse: 0.5223 - val_loss: 1.5950 - val_mse: 0.5664\n",
            "Epoch 29/150\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 4.801249739316103e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.5538 - mse: 0.5266 - val_loss: 1.5904 - val_mse: 0.5671\n",
            "Epoch 30/150\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 4.787366452787259e-05.\n",
            "16/16 [==============================] - 14s 871ms/step - loss: 1.5409 - mse: 0.5190 - val_loss: 1.5938 - val_mse: 0.5758\n",
            "Epoch 31/150\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 4.773047418521155e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.5344 - mse: 0.5177 - val_loss: 1.5616 - val_mse: 0.5489\n",
            "Epoch 32/150\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 4.758296756247637e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.5278 - mse: 0.5163 - val_loss: 1.5630 - val_mse: 0.5555\n",
            "Epoch 33/150\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 4.743118946691358e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.5223 - mse: 0.5162 - val_loss: 1.5501 - val_mse: 0.5479\n",
            "Epoch 34/150\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 4.7275181061892785e-05.\n",
            "16/16 [==============================] - 14s 869ms/step - loss: 1.5113 - mse: 0.5104 - val_loss: 1.5522 - val_mse: 0.5553\n",
            "Epoch 35/150\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 4.711499074567721e-05.\n",
            "16/16 [==============================] - 14s 871ms/step - loss: 1.4824 - mse: 0.4867 - val_loss: 1.5344 - val_mse: 0.5426\n",
            "Epoch 36/150\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 4.695066327194803e-05.\n",
            "16/16 [==============================] - 14s 870ms/step - loss: 1.4890 - mse: 0.4985 - val_loss: 1.5280 - val_mse: 0.5414\n",
            "Epoch 37/150\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 4.67822470014782e-05.\n",
            "16/16 [==============================] - 14s 869ms/step - loss: 1.4894 - mse: 0.5041 - val_loss: 1.5391 - val_mse: 0.5575\n",
            "Epoch 38/150\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 4.660979027576767e-05.\n",
            "16/16 [==============================] - 14s 871ms/step - loss: 1.4745 - mse: 0.4942 - val_loss: 1.5206 - val_mse: 0.5441\n",
            "Epoch 39/150\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 4.6433345041259854e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.4671 - mse: 0.4920 - val_loss: 1.5305 - val_mse: 0.5592\n",
            "Epoch 40/150\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 4.625295959985445e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.4572 - mse: 0.4872 - val_loss: 1.5081 - val_mse: 0.5420\n",
            "Epoch 41/150\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 4.606868585769228e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.4550 - mse: 0.4900 - val_loss: 1.5094 - val_mse: 0.5482\n",
            "Epoch 42/150\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 4.5880575700236467e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.4439 - mse: 0.4839 - val_loss: 1.5021 - val_mse: 0.5459\n",
            "Epoch 43/150\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 4.568868461504595e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.4515 - mse: 0.4966 - val_loss: 1.4951 - val_mse: 0.5438\n",
            "Epoch 44/150\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 4.5493064445177215e-05.\n",
            "16/16 [==============================] - 14s 872ms/step - loss: 1.4207 - mse: 0.4707 - val_loss: 1.4890 - val_mse: 0.5427\n",
            "Epoch 45/150\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 4.529377063508218e-05.\n",
            "16/16 [==============================] - 14s 872ms/step - loss: 1.4362 - mse: 0.4911 - val_loss: 1.4801 - val_mse: 0.5388\n",
            "Epoch 46/150\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 4.509086222881558e-05.\n",
            "16/16 [==============================] - 14s 872ms/step - loss: 1.4272 - mse: 0.4871 - val_loss: 1.4813 - val_mse: 0.5449\n",
            "Epoch 47/150\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 4.4884394625601943e-05.\n",
            "16/16 [==============================] - 14s 870ms/step - loss: 1.4114 - mse: 0.4762 - val_loss: 1.4779 - val_mse: 0.5463\n",
            "Epoch 48/150\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 4.467442320260976e-05.\n",
            "16/16 [==============================] - 14s 870ms/step - loss: 1.3942 - mse: 0.4638 - val_loss: 1.4671 - val_mse: 0.5404\n",
            "Epoch 49/150\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 4.446101055616234e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.4047 - mse: 0.4792 - val_loss: 1.4590 - val_mse: 0.5371\n",
            "Epoch 50/150\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 4.424421563742692e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.3775 - mse: 0.4568 - val_loss: 1.4587 - val_mse: 0.5417\n",
            "Epoch 51/150\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 4.4024093754228153e-05.\n",
            "16/16 [==============================] - 14s 884ms/step - loss: 1.3891 - mse: 0.4732 - val_loss: 1.4485 - val_mse: 0.5361\n",
            "Epoch 52/150\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 4.380071105093761e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.3762 - mse: 0.4650 - val_loss: 1.4439 - val_mse: 0.5363\n",
            "Epoch 53/150\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 4.3574126407287645e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.3911 - mse: 0.4846 - val_loss: 1.4347 - val_mse: 0.5317\n",
            "Epoch 54/150\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 4.334440229838202e-05.\n",
            "16/16 [==============================] - 14s 869ms/step - loss: 1.3694 - mse: 0.4675 - val_loss: 1.4344 - val_mse: 0.5360\n",
            "Epoch 55/150\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 4.311160117446871e-05.\n",
            "16/16 [==============================] - 14s 871ms/step - loss: 1.3728 - mse: 0.4756 - val_loss: 1.4199 - val_mse: 0.5261\n",
            "Epoch 56/150\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 4.2875785460952185e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.3468 - mse: 0.4542 - val_loss: 1.4247 - val_mse: 0.5357\n",
            "Epoch 57/150\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 4.263701755840586e-05.\n",
            "16/16 [==============================] - 14s 883ms/step - loss: 1.3512 - mse: 0.4633 - val_loss: 1.4138 - val_mse: 0.5294\n",
            "Epoch 58/150\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 4.2395363459944206e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.3414 - mse: 0.4581 - val_loss: 1.4025 - val_mse: 0.5228\n",
            "Epoch 59/150\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 4.215088913243669e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.3301 - mse: 0.4515 - val_loss: 1.3981 - val_mse: 0.5230\n",
            "Epoch 60/150\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 4.190365689988016e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.3211 - mse: 0.4471 - val_loss: 1.3960 - val_mse: 0.5255\n",
            "Epoch 61/150\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 4.1653736294052796e-05.\n",
            "16/16 [==============================] - 14s 872ms/step - loss: 1.3232 - mse: 0.4538 - val_loss: 1.3885 - val_mse: 0.5224\n",
            "Epoch 62/150\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 4.1401189587245336e-05.\n",
            "16/16 [==============================] - 14s 872ms/step - loss: 1.3248 - mse: 0.4598 - val_loss: 1.3925 - val_mse: 0.5308\n",
            "Epoch 63/150\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 4.114608264255538e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.3093 - mse: 0.4487 - val_loss: 1.3838 - val_mse: 0.5265\n",
            "Epoch 64/150\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 4.088848491210328e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.3081 - mse: 0.4519 - val_loss: 1.3905 - val_mse: 0.5375\n",
            "Epoch 65/150\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 4.0628462205561386e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.2988 - mse: 0.4469 - val_loss: 1.3726 - val_mse: 0.5240\n",
            "Epoch 66/150\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 4.0366083920932845e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.2894 - mse: 0.4418 - val_loss: 1.3765 - val_mse: 0.5321\n",
            "Epoch 67/150\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 4.0101415814518025e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.3227 - mse: 0.4793 - val_loss: 1.3605 - val_mse: 0.5203\n",
            "Epoch 68/150\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 3.9834523616489826e-05.\n",
            "16/16 [==============================] - 14s 880ms/step - loss: 1.2848 - mse: 0.4457 - val_loss: 1.3500 - val_mse: 0.5142\n",
            "Epoch 69/150\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 3.95654766443143e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.2719 - mse: 0.4372 - val_loss: 1.3477 - val_mse: 0.5164\n",
            "Epoch 70/150\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 3.9294344187920515e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.2549 - mse: 0.4246 - val_loss: 1.3476 - val_mse: 0.5204\n",
            "Epoch 71/150\n",
            "\n",
            "Epoch 00071: LearningRateScheduler reducing learning rate to 3.902119550971424e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.2662 - mse: 0.4400 - val_loss: 1.3430 - val_mse: 0.5200\n",
            "Epoch 72/150\n",
            "\n",
            "Epoch 00072: LearningRateScheduler reducing learning rate to 3.874609984459157e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.2670 - mse: 0.4450 - val_loss: 1.3399 - val_mse: 0.5211\n",
            "Epoch 73/150\n",
            "\n",
            "Epoch 00073: LearningRateScheduler reducing learning rate to 3.8469122787980054e-05.\n",
            "16/16 [==============================] - 14s 872ms/step - loss: 1.2471 - mse: 0.4293 - val_loss: 1.3304 - val_mse: 0.5157\n",
            "Epoch 74/150\n",
            "\n",
            "Epoch 00074: LearningRateScheduler reducing learning rate to 3.819033352087321e-05.\n",
            "16/16 [==============================] - 14s 870ms/step - loss: 1.2406 - mse: 0.4269 - val_loss: 1.3303 - val_mse: 0.5198\n",
            "Epoch 75/150\n",
            "\n",
            "Epoch 00075: LearningRateScheduler reducing learning rate to 3.790980119679544e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.2533 - mse: 0.4437 - val_loss: 1.3278 - val_mse: 0.5211\n",
            "Epoch 76/150\n",
            "\n",
            "Epoch 00076: LearningRateScheduler reducing learning rate to 3.762759494181565e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.2506 - mse: 0.4449 - val_loss: 1.3316 - val_mse: 0.5288\n",
            "Epoch 77/150\n",
            "\n",
            "Epoch 00077: LearningRateScheduler reducing learning rate to 3.734378385456089e-05.\n",
            "16/16 [==============================] - 14s 880ms/step - loss: 1.2382 - mse: 0.4364 - val_loss: 1.3239 - val_mse: 0.5252\n",
            "Epoch 78/150\n",
            "\n",
            "Epoch 00078: LearningRateScheduler reducing learning rate to 3.7058433396049514e-05.\n",
            "16/16 [==============================] - 14s 880ms/step - loss: 1.2255 - mse: 0.4278 - val_loss: 1.3183 - val_mse: 0.5236\n",
            "Epoch 79/150\n",
            "\n",
            "Epoch 00079: LearningRateScheduler reducing learning rate to 3.677161622096255e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.2255 - mse: 0.4318 - val_loss: 1.3046 - val_mse: 0.5138\n",
            "Epoch 80/150\n",
            "\n",
            "Epoch 00080: LearningRateScheduler reducing learning rate to 3.6483397736218814e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.2168 - mse: 0.4270 - val_loss: 1.3028 - val_mse: 0.5159\n",
            "Epoch 81/150\n",
            "\n",
            "Epoch 00081: LearningRateScheduler reducing learning rate to 3.6193846931887995e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.2036 - mse: 0.4177 - val_loss: 1.3003 - val_mse: 0.5172\n",
            "Epoch 82/150\n",
            "\n",
            "Epoch 00082: LearningRateScheduler reducing learning rate to 3.590303277066547e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.2251 - mse: 0.4430 - val_loss: 1.2866 - val_mse: 0.5072\n",
            "Epoch 83/150\n",
            "\n",
            "Epoch 00083: LearningRateScheduler reducing learning rate to 3.56110241878859e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.2055 - mse: 0.4271 - val_loss: 1.2868 - val_mse: 0.5111\n",
            "Epoch 84/150\n",
            "\n",
            "Epoch 00084: LearningRateScheduler reducing learning rate to 3.531788648350465e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.2101 - mse: 0.4354 - val_loss: 1.2950 - val_mse: 0.5230\n",
            "Epoch 85/150\n",
            "\n",
            "Epoch 00085: LearningRateScheduler reducing learning rate to 3.502368853924902e-05.\n",
            "16/16 [==============================] - 14s 880ms/step - loss: 1.2009 - mse: 0.4298 - val_loss: 1.2673 - val_mse: 0.4990\n",
            "Epoch 86/150\n",
            "\n",
            "Epoch 00086: LearningRateScheduler reducing learning rate to 3.472849560220923e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.2069 - mse: 0.4396 - val_loss: 1.2782 - val_mse: 0.5137\n",
            "Epoch 87/150\n",
            "\n",
            "Epoch 00087: LearningRateScheduler reducing learning rate to 3.443237650055817e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.1950 - mse: 0.4314 - val_loss: 1.2671 - val_mse: 0.5063\n",
            "Epoch 88/150\n",
            "\n",
            "Epoch 00088: LearningRateScheduler reducing learning rate to 3.413540003517492e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.1802 - mse: 0.4203 - val_loss: 1.2668 - val_mse: 0.5097\n",
            "Epoch 89/150\n",
            "\n",
            "Epoch 00089: LearningRateScheduler reducing learning rate to 3.383762776717052e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.1806 - mse: 0.4243 - val_loss: 1.2486 - val_mse: 0.4950\n",
            "Epoch 90/150\n",
            "\n",
            "Epoch 00090: LearningRateScheduler reducing learning rate to 3.3539128445021666e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.1714 - mse: 0.4187 - val_loss: 1.2621 - val_mse: 0.5120\n",
            "Epoch 91/150\n",
            "\n",
            "Epoch 00091: LearningRateScheduler reducing learning rate to 3.323996718442188e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.1826 - mse: 0.4334 - val_loss: 1.2509 - val_mse: 0.5043\n",
            "Epoch 92/150\n",
            "\n",
            "Epoch 00092: LearningRateScheduler reducing learning rate to 3.2940212680425036e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.1726 - mse: 0.4269 - val_loss: 1.2533 - val_mse: 0.5102\n",
            "Epoch 93/150\n",
            "\n",
            "Epoch 00093: LearningRateScheduler reducing learning rate to 3.263992639122847e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.1819 - mse: 0.4396 - val_loss: 1.2474 - val_mse: 0.5076\n",
            "Epoch 94/150\n",
            "\n",
            "Epoch 00094: LearningRateScheduler reducing learning rate to 3.23391733551301e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.1612 - mse: 0.4223 - val_loss: 1.2411 - val_mse: 0.5047\n",
            "Epoch 95/150\n",
            "\n",
            "Epoch 00095: LearningRateScheduler reducing learning rate to 3.203801498055452e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.1541 - mse: 0.4186 - val_loss: 1.2449 - val_mse: 0.5120\n",
            "Epoch 96/150\n",
            "\n",
            "Epoch 00096: LearningRateScheduler reducing learning rate to 3.1736519859080604e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.1458 - mse: 0.4137 - val_loss: 1.2349 - val_mse: 0.5054\n",
            "Epoch 97/150\n",
            "\n",
            "Epoch 00097: LearningRateScheduler reducing learning rate to 3.1434745744952604e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.1500 - mse: 0.4212 - val_loss: 1.2391 - val_mse: 0.5128\n",
            "Epoch 98/150\n",
            "\n",
            "Epoch 00098: LearningRateScheduler reducing learning rate to 3.113275757559382e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.1582 - mse: 0.4327 - val_loss: 1.2307 - val_mse: 0.5076\n",
            "Epoch 99/150\n",
            "\n",
            "Epoch 00099: LearningRateScheduler reducing learning rate to 3.083061666003209e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.1352 - mse: 0.4130 - val_loss: 1.2271 - val_mse: 0.5072\n",
            "Epoch 100/150\n",
            "\n",
            "Epoch 00100: LearningRateScheduler reducing learning rate to 3.0528384283012004e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.1314 - mse: 0.4123 - val_loss: 1.2238 - val_mse: 0.5072\n",
            "Epoch 101/150\n",
            "\n",
            "Epoch 00101: LearningRateScheduler reducing learning rate to 3.0226121705006992e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.1339 - mse: 0.4180 - val_loss: 1.2249 - val_mse: 0.5113\n",
            "Epoch 102/150\n",
            "\n",
            "Epoch 00102: LearningRateScheduler reducing learning rate to 2.992389016223131e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.1287 - mse: 0.4159 - val_loss: 1.2175 - val_mse: 0.5071\n",
            "Epoch 103/150\n",
            "\n",
            "Epoch 00103: LearningRateScheduler reducing learning rate to 2.9621749066029e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.1221 - mse: 0.4124 - val_loss: 1.2100 - val_mse: 0.5027\n",
            "Epoch 104/150\n",
            "\n",
            "Epoch 00104: LearningRateScheduler reducing learning rate to 2.9319756003777008e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.1151 - mse: 0.4086 - val_loss: 1.2098 - val_mse: 0.5055\n",
            "Epoch 105/150\n",
            "\n",
            "Epoch 00105: LearningRateScheduler reducing learning rate to 2.9017968540054453e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.1279 - mse: 0.4244 - val_loss: 1.2068 - val_mse: 0.5056\n",
            "Epoch 106/150\n",
            "\n",
            "Epoch 00106: LearningRateScheduler reducing learning rate to 2.8716446016742336e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.1155 - mse: 0.4150 - val_loss: 1.2055 - val_mse: 0.5072\n",
            "Epoch 107/150\n",
            "\n",
            "Epoch 00107: LearningRateScheduler reducing learning rate to 2.8415244152413176e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.1049 - mse: 0.4073 - val_loss: 1.2012 - val_mse: 0.5058\n",
            "Epoch 108/150\n",
            "\n",
            "Epoch 00108: LearningRateScheduler reducing learning rate to 2.811442044332028e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.1006 - mse: 0.4059 - val_loss: 1.2004 - val_mse: 0.5080\n",
            "Epoch 109/150\n",
            "\n",
            "Epoch 00109: LearningRateScheduler reducing learning rate to 2.7814028763855753e-05.\n",
            "16/16 [==============================] - 14s 880ms/step - loss: 1.0938 - mse: 0.4021 - val_loss: 1.1949 - val_mse: 0.5055\n",
            "Epoch 110/150\n",
            "\n",
            "Epoch 00110: LearningRateScheduler reducing learning rate to 2.7514124766470495e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.1008 - mse: 0.4121 - val_loss: 1.1876 - val_mse: 0.5010\n",
            "Epoch 111/150\n",
            "\n",
            "Epoch 00111: LearningRateScheduler reducing learning rate to 2.721476228239842e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.0888 - mse: 0.4029 - val_loss: 1.1801 - val_mse: 0.4964\n",
            "Epoch 112/150\n",
            "\n",
            "Epoch 00112: LearningRateScheduler reducing learning rate to 2.6915995121577327e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.0949 - mse: 0.4119 - val_loss: 1.1828 - val_mse: 0.5019\n",
            "Epoch 113/150\n",
            "\n",
            "Epoch 00113: LearningRateScheduler reducing learning rate to 2.661787527381708e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.0905 - mse: 0.4102 - val_loss: 1.1767 - val_mse: 0.4986\n",
            "Epoch 114/150\n",
            "\n",
            "Epoch 00114: LearningRateScheduler reducing learning rate to 2.632045470836398e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.0801 - mse: 0.4027 - val_loss: 1.1820 - val_mse: 0.5066\n",
            "Epoch 115/150\n",
            "\n",
            "Epoch 00115: LearningRateScheduler reducing learning rate to 2.6023783575424277e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.0883 - mse: 0.4136 - val_loss: 1.1668 - val_mse: 0.4940\n",
            "Epoch 116/150\n",
            "\n",
            "Epoch 00116: LearningRateScheduler reducing learning rate to 2.5727912005372216e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.0805 - mse: 0.4084 - val_loss: 1.1757 - val_mse: 0.5057\n",
            "Epoch 117/150\n",
            "\n",
            "Epoch 00117: LearningRateScheduler reducing learning rate to 2.5432890108759824e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.0683 - mse: 0.3988 - val_loss: 1.1627 - val_mse: 0.4953\n",
            "Epoch 118/150\n",
            "\n",
            "Epoch 00118: LearningRateScheduler reducing learning rate to 2.5138766178373366e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.0680 - mse: 0.4012 - val_loss: 1.1742 - val_mse: 0.5096\n",
            "Epoch 119/150\n",
            "\n",
            "Epoch 00119: LearningRateScheduler reducing learning rate to 2.484558848790728e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.0733 - mse: 0.4092 - val_loss: 1.1621 - val_mse: 0.5000\n",
            "Epoch 120/150\n",
            "\n",
            "Epoch 00120: LearningRateScheduler reducing learning rate to 2.4553403494375607e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.0596 - mse: 0.3981 - val_loss: 1.1625 - val_mse: 0.5030\n",
            "Epoch 121/150\n",
            "\n",
            "Epoch 00121: LearningRateScheduler reducing learning rate to 2.426225583900959e-05.\n",
            "16/16 [==============================] - 14s 872ms/step - loss: 1.0680 - mse: 0.4091 - val_loss: 1.1525 - val_mse: 0.4956\n",
            "Epoch 122/150\n",
            "\n",
            "Epoch 00122: LearningRateScheduler reducing learning rate to 2.3972191942640208e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.0570 - mse: 0.4007 - val_loss: 1.1525 - val_mse: 0.4980\n",
            "Epoch 123/150\n",
            "\n",
            "Epoch 00123: LearningRateScheduler reducing learning rate to 2.3683256410688716e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.0644 - mse: 0.4106 - val_loss: 1.1515 - val_mse: 0.4995\n",
            "Epoch 124/150\n",
            "\n",
            "Epoch 00124: LearningRateScheduler reducing learning rate to 2.3395492034063303e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.0551 - mse: 0.4037 - val_loss: 1.1513 - val_mse: 0.5018\n",
            "Epoch 125/150\n",
            "\n",
            "Epoch 00125: LearningRateScheduler reducing learning rate to 2.3108941586765436e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.0585 - mse: 0.4095 - val_loss: 1.1651 - val_mse: 0.5179\n",
            "Epoch 126/150\n",
            "\n",
            "Epoch 00126: LearningRateScheduler reducing learning rate to 2.282364602936547e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.0409 - mse: 0.3943 - val_loss: 1.1411 - val_mse: 0.4963\n",
            "Epoch 127/150\n",
            "\n",
            "Epoch 00127: LearningRateScheduler reducing learning rate to 2.25396463062534e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.0386 - mse: 0.3943 - val_loss: 1.1438 - val_mse: 0.5012\n",
            "Epoch 128/150\n",
            "\n",
            "Epoch 00128: LearningRateScheduler reducing learning rate to 2.2256983345646865e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.0309 - mse: 0.3889 - val_loss: 1.1382 - val_mse: 0.4980\n",
            "Epoch 129/150\n",
            "\n",
            "Epoch 00129: LearningRateScheduler reducing learning rate to 2.1975694467597915e-05.\n",
            "16/16 [==============================] - 14s 880ms/step - loss: 1.0391 - mse: 0.3995 - val_loss: 1.1476 - val_mse: 0.5097\n",
            "Epoch 130/150\n",
            "\n",
            "Epoch 00130: LearningRateScheduler reducing learning rate to 2.169581877324399e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.0422 - mse: 0.4048 - val_loss: 1.1372 - val_mse: 0.5015\n",
            "Epoch 131/150\n",
            "\n",
            "Epoch 00131: LearningRateScheduler reducing learning rate to 2.1417393552636784e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.0344 - mse: 0.3993 - val_loss: 1.1326 - val_mse: 0.4992\n",
            "Epoch 132/150\n",
            "\n",
            "Epoch 00132: LearningRateScheduler reducing learning rate to 2.1140454285636074e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.0285 - mse: 0.3956 - val_loss: 1.1345 - val_mse: 0.5033\n",
            "Epoch 133/150\n",
            "\n",
            "Epoch 00133: LearningRateScheduler reducing learning rate to 2.0865036438094605e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.0346 - mse: 0.4039 - val_loss: 1.1313 - val_mse: 0.5023\n",
            "Epoch 134/150\n",
            "\n",
            "Epoch 00134: LearningRateScheduler reducing learning rate to 2.0591173666750583e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.0291 - mse: 0.4006 - val_loss: 1.1260 - val_mse: 0.4990\n",
            "Epoch 135/150\n",
            "\n",
            "Epoch 00135: LearningRateScheduler reducing learning rate to 2.031889961505754e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 1.0353 - mse: 0.4088 - val_loss: 1.1332 - val_mse: 0.5083\n",
            "Epoch 136/150\n",
            "\n",
            "Epoch 00136: LearningRateScheduler reducing learning rate to 2.0048247913190873e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.0279 - mse: 0.4036 - val_loss: 1.1242 - val_mse: 0.5015\n",
            "Epoch 137/150\n",
            "\n",
            "Epoch 00137: LearningRateScheduler reducing learning rate to 1.9779250383471347e-05.\n",
            "16/16 [==============================] - 14s 875ms/step - loss: 1.0146 - mse: 0.3923 - val_loss: 1.1179 - val_mse: 0.4972\n",
            "Epoch 138/150\n",
            "\n",
            "Epoch 00138: LearningRateScheduler reducing learning rate to 1.9511937041256766e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.0250 - mse: 0.4047 - val_loss: 1.1150 - val_mse: 0.4962\n",
            "Epoch 139/150\n",
            "\n",
            "Epoch 00139: LearningRateScheduler reducing learning rate to 1.9246337890062238e-05.\n",
            "16/16 [==============================] - 14s 876ms/step - loss: 1.0161 - mse: 0.3978 - val_loss: 1.1138 - val_mse: 0.4970\n",
            "Epoch 140/150\n",
            "\n",
            "Epoch 00140: LearningRateScheduler reducing learning rate to 1.8982481127513907e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.0163 - mse: 0.4000 - val_loss: 1.1093 - val_mse: 0.4945\n",
            "Epoch 141/150\n",
            "\n",
            "Epoch 00141: LearningRateScheduler reducing learning rate to 1.8720394940114592e-05.\n",
            "16/16 [==============================] - 14s 879ms/step - loss: 1.0120 - mse: 0.3977 - val_loss: 1.1119 - val_mse: 0.4990\n",
            "Epoch 142/150\n",
            "\n",
            "Epoch 00142: LearningRateScheduler reducing learning rate to 1.8460107503249286e-05.\n",
            "16/16 [==============================] - 14s 883ms/step - loss: 1.0201 - mse: 0.4078 - val_loss: 1.1088 - val_mse: 0.4980\n",
            "Epoch 143/150\n",
            "\n",
            "Epoch 00143: LearningRateScheduler reducing learning rate to 1.8201643394147808e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.0075 - mse: 0.3971 - val_loss: 1.1099 - val_mse: 0.5011\n",
            "Epoch 144/150\n",
            "\n",
            "Epoch 00144: LearningRateScheduler reducing learning rate to 1.794502897369227e-05.\n",
            "16/16 [==============================] - 14s 873ms/step - loss: 1.0008 - mse: 0.3924 - val_loss: 1.1076 - val_mse: 0.5007\n",
            "Epoch 145/150\n",
            "\n",
            "Epoch 00145: LearningRateScheduler reducing learning rate to 1.7690288799202324e-05.\n",
            "16/16 [==============================] - 14s 877ms/step - loss: 1.0161 - mse: 0.4097 - val_loss: 1.1104 - val_mse: 0.5053\n",
            "Epoch 146/150\n",
            "\n",
            "Epoch 00146: LearningRateScheduler reducing learning rate to 1.743744562532406e-05.\n",
            "16/16 [==============================] - 14s 871ms/step - loss: 0.9927 - mse: 0.3880 - val_loss: 1.1051 - val_mse: 0.5018\n",
            "Epoch 147/150\n",
            "\n",
            "Epoch 00147: LearningRateScheduler reducing learning rate to 1.7186522197732678e-05.\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 0.9934 - mse: 0.3905 - val_loss: 1.0976 - val_mse: 0.4962\n",
            "Epoch 148/150\n",
            "\n",
            "Epoch 00148: LearningRateScheduler reducing learning rate to 1.6937539460499296e-05.\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 1.0061 - mse: 0.4051 - val_loss: 1.0918 - val_mse: 0.4921\n",
            "Epoch 149/150\n",
            "\n",
            "Epoch 00149: LearningRateScheduler reducing learning rate to 1.6690520141900553e-05.\n",
            "16/16 [==============================] - 14s 883ms/step - loss: 0.9906 - mse: 0.3912 - val_loss: 1.1000 - val_mse: 0.5018\n",
            "Epoch 150/150\n",
            "\n",
            "Epoch 00150: LearningRateScheduler reducing learning rate to 1.644548158440216e-05.\n",
            "16/16 [==============================] - 14s 884ms/step - loss: 0.9858 - mse: 0.3880 - val_loss: 1.0932 - val_mse: 0.4967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z38S64ZpeB_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "dc934d0b-8fa8-43d5-d5c7-ffc5080ddbf7"
      },
      "source": [
        "plt.plot(history.history['mse'])\n",
        "plt.plot(history.history['val_mse'])\n",
        "plt.title('Model MSE')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "osgMWhxRIibD",
        "outputId": "caad439e-4cb3-4f62-df1b-3b0bea9e1f72"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYEHkn7jBD03"
      },
      "source": [
        "# initial_learning_rate = 0.0001\n",
        "# epochs = 100\n",
        "# decay = initial_learning_rate / epochs\n",
        "# def lr_time_based_decay(epoch, lr):\n",
        "#     lr = lr * 1 / (1 + decay * epoch)\n",
        "#     return lr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NqyrbTTA1_W"
      },
      "source": [
        "# history = model.fit(x_train, y_train_scaled,\n",
        "#           epochs=100,\n",
        "#           batch_size = 128,\n",
        "#           validation_split=0.3,\n",
        "#           callbacks=[callbacks.LearningRateScheduler(lr_time_based_decay, verbose=1)],\n",
        "#           verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM9oEpO0RSVZ"
      },
      "source": [
        "for i in range(len(y_test_scaled.iloc[:,0])):     #unscaling y_test_scaled back to original distribution\n",
        "  # print(i)\n",
        "  y_test_scaled.iloc[i,0] = ((y_test_scaled.iloc[i,0] + 1)*(maxb-minb))/2 + minb\n",
        "\n",
        "\n",
        "for i in range(len(y_test_scaled.iloc[:,1])):\n",
        "  # print(i)\n",
        "  y_test_scaled.iloc[i,1] = ((y_test_scaled.iloc[i,1] + 1)*(maxr-minr))/2 + minr   #\n",
        "\n",
        "for i in range(len(y_test_scaled.iloc[:,2])):\n",
        "  # print(i)\n",
        "  y_test_scaled.iloc[i,2] = ((y_test_scaled.iloc[i,2] + 1)*(maxt-mint))/2 + mint \n",
        "\n",
        "for i in range(len(y_test_scaled.iloc[:,3])):\n",
        "  # print(i)\n",
        "  y_test_scaled.iloc[i,3] = ((y_test_scaled.iloc[i,3] + 1)*(maxx-minx))/2 + minx   #\n",
        "\n",
        "for i in range(len(y_test_scaled.iloc[:,4])):\n",
        "  # print(i)\n",
        "  y_test_scaled.iloc[i,4] = ((y_test_scaled.iloc[i,4] + 1)*(maxy-miny))/2 + miny "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciu3jV4hR5C-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "b7bff7b4-36ac-4e10-8faa-a6bf29574362"
      },
      "source": [
        "y_test_scaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B</th>\n",
              "      <th>R</th>\n",
              "      <th>Theta</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2893</th>\n",
              "      <td>2220.0</td>\n",
              "      <td>6912.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3055</th>\n",
              "      <td>1420.0</td>\n",
              "      <td>-8064.0</td>\n",
              "      <td>-4000.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2155</th>\n",
              "      <td>3020.0</td>\n",
              "      <td>-8064.0</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>52.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>2820.0</td>\n",
              "      <td>-6912.0</td>\n",
              "      <td>-4000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665</th>\n",
              "      <td>2020.0</td>\n",
              "      <td>3456.0</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3874</th>\n",
              "      <td>1220.0</td>\n",
              "      <td>13824.0</td>\n",
              "      <td>8000.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3552</th>\n",
              "      <td>3220.0</td>\n",
              "      <td>-11520.0</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619</th>\n",
              "      <td>1620.0</td>\n",
              "      <td>8064.0</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>3220.0</td>\n",
              "      <td>2304.0</td>\n",
              "      <td>-4000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2555</th>\n",
              "      <td>1820.0</td>\n",
              "      <td>-8064.0</td>\n",
              "      <td>8000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>52.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1238 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           B        R   Theta     X     Y\n",
              "2893  2220.0   6912.0     0.0  52.0  50.0\n",
              "3055  1420.0  -8064.0 -4000.0  52.0  50.0\n",
              "2155  3020.0  -8064.0  4000.0  50.0  52.0\n",
              "481   2820.0  -6912.0 -4000.0  50.0  50.0\n",
              "665   2020.0   3456.0  4000.0  50.0  50.0\n",
              "...      ...      ...     ...   ...   ...\n",
              "3874  1220.0  13824.0  8000.0  52.0  50.0\n",
              "3552  3220.0 -11520.0  4000.0  52.0  50.0\n",
              "619   1620.0   8064.0  4000.0  50.0  50.0\n",
              "539   3220.0   2304.0 -4000.0  50.0  50.0\n",
              "2555  1820.0  -8064.0  8000.0  50.0  52.0\n",
              "\n",
              "[1238 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWUwm9HmSbBu"
      },
      "source": [
        "prediction = model.predict(x_test)    #finding predictions on x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD6VeZttjmDO"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUQl7xuXSg53"
      },
      "source": [
        "for i in range(len(prediction[:,0])):     #unscaling prediction back to original distribution\n",
        "  # print(i)\n",
        "  prediction[i,0] = ((prediction[i,0] + 1)*(maxb-minb))/2 + minb\n",
        "\n",
        "\n",
        "for i in range(len(prediction[:,1])):\n",
        "  # print(i)\n",
        "  prediction[i,1] = ((prediction[i,1] + 1)*(maxr-minr))/2 + minr   #\n",
        "\n",
        "for i in range(len(prediction[:,2])):\n",
        "  # print(i)\n",
        "  prediction[i,2] = ((prediction[i,2] + 1)*(maxt-mint))/2 + mint \n",
        "\n",
        "for i in range(len(prediction[:,3])):\n",
        "  # print(i)\n",
        "  prediction[i,3] = ((prediction[i,3] + 1)*(maxx-minx))/2 + minx   #\n",
        "\n",
        "for i in range(len(prediction[:,4])):\n",
        "  # print(i)\n",
        "  prediction[i,4] = ((prediction[i,4] + 1)*(maxy-miny))/2 + miny "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9settPVSmmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc58bc1b-b087-4414-f7c8-802d002c8421"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 18.704205 ,   8.268025 ,   6.6444325,  17.379068 ,  16.510143 ],\n",
              "       [ 20.17179  ,   1.351907 , -19.947721 ,  16.17385  ,  16.013565 ],\n",
              "       [ 18.147415 ,   2.2511263,  -3.1462283,  16.436237 ,  17.610424 ],\n",
              "       ...,\n",
              "       [ 17.869741 ,   7.73464  ,   1.8196359,  16.623913 ,  16.534464 ],\n",
              "       [ 22.0707   ,   0.2343053,  -4.8081083,  16.716135 ,  16.484648 ],\n",
              "       [ 20.015505 ,   8.783417 ,   6.0361223,  16.76498  ,  17.418926 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi7rCA1ATByn"
      },
      "source": [
        "prediction_round = np.rint(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgEcPPOvTIJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6570ef7-6fbc-4f07-c8d8-494c5a7390b1"
      },
      "source": [
        "prediction_round"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 19.,   8.,   7.,  17.,  17.],\n",
              "       [ 20.,   1., -20.,  16.,  16.],\n",
              "       [ 18.,   2.,  -3.,  16.,  18.],\n",
              "       ...,\n",
              "       [ 18.,   8.,   2.,  17.,  17.],\n",
              "       [ 22.,   0.,  -5.,  17.,  16.],\n",
              "       [ 20.,   9.,   6.,  17.,  17.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "UR6iyR6x12W2",
        "outputId": "fcce2857-ce4f-49d5-cbea-ca19de9aeed4"
      },
      "source": [
        "y_test_scaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B</th>\n",
              "      <th>R</th>\n",
              "      <th>Theta</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2893</th>\n",
              "      <td>2220.0</td>\n",
              "      <td>6912.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3055</th>\n",
              "      <td>1420.0</td>\n",
              "      <td>-8064.0</td>\n",
              "      <td>-4000.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2155</th>\n",
              "      <td>3020.0</td>\n",
              "      <td>-8064.0</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>52.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>2820.0</td>\n",
              "      <td>-6912.0</td>\n",
              "      <td>-4000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665</th>\n",
              "      <td>2020.0</td>\n",
              "      <td>3456.0</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3874</th>\n",
              "      <td>1220.0</td>\n",
              "      <td>13824.0</td>\n",
              "      <td>8000.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3552</th>\n",
              "      <td>3220.0</td>\n",
              "      <td>-11520.0</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619</th>\n",
              "      <td>1620.0</td>\n",
              "      <td>8064.0</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>3220.0</td>\n",
              "      <td>2304.0</td>\n",
              "      <td>-4000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2555</th>\n",
              "      <td>1820.0</td>\n",
              "      <td>-8064.0</td>\n",
              "      <td>8000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>52.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1238 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           B        R   Theta     X     Y\n",
              "2893  2220.0   6912.0     0.0  52.0  50.0\n",
              "3055  1420.0  -8064.0 -4000.0  52.0  50.0\n",
              "2155  3020.0  -8064.0  4000.0  50.0  52.0\n",
              "481   2820.0  -6912.0 -4000.0  50.0  50.0\n",
              "665   2020.0   3456.0  4000.0  50.0  50.0\n",
              "...      ...      ...     ...   ...   ...\n",
              "3874  1220.0  13824.0  8000.0  52.0  50.0\n",
              "3552  3220.0 -11520.0  4000.0  52.0  50.0\n",
              "619   1620.0   8064.0  4000.0  50.0  50.0\n",
              "539   3220.0   2304.0 -4000.0  50.0  50.0\n",
              "2555  1820.0  -8064.0  8000.0  50.0  52.0\n",
              "\n",
              "[1238 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tp_R8uZLMml"
      },
      "source": [
        "# y_test_inv = target_scaler.inverse_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP0uoETYLPsX"
      },
      "source": [
        "# prediction = model.predict(x_test)\n",
        "# prediction = target_scaler.inverse_transform(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnB_UDyJLSyk"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgdKp4DRLVb-"
      },
      "source": [
        "mse = mean_squared_error(y_test_scaled, prediction)\n",
        "mae = mean_absolute_error(y_test_scaled, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5Cbf9ALYUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f827aa3e-7990-4d59-8008-1acc40718011"
      },
      "source": [
        "print(mse)\n",
        "print(mae)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20737821.17114968\n",
            "2816.478427997175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udXes8FKM4eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dfe61e3-0262-45f8-dd0c-0a3db92ef4af"
      },
      "source": [
        "model.save(\"absolute-best-model6\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: absolute-best-model4/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHWLOTn51sIA"
      },
      "source": [
        "model.save(\"absolute-best-model6.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XkC4i0v3pKH"
      },
      "source": [
        "# np.savetxt('absolute-best-model3-predictions.csv', prediction_round, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecdzdeZd3sgC"
      },
      "source": [
        "# np.savetxt('ground-truth--1to1-rxy.csv', y_test_scaled, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXmWSHqtk2Nz"
      },
      "source": [
        "# b1 = y_test_inv[:,0]\n",
        "# b2 = prediction_round[:,0]\n",
        "# error_b = mean_absolute_error(b1, b2)\n",
        "# print(error_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kEyoMZRmksi"
      },
      "source": [
        "# r1 = y_test_scaled.iloc[:,0]\n",
        "# r2 = prediction_round[:,0]\n",
        "# error_r = mean_absolute_error(r1, r2)\n",
        "# print(error_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbTZbN0lmsWL"
      },
      "source": [
        "# t1 = y_test_inv[:,2]\n",
        "# t2 = prediction_round[:,2]\n",
        "# error_t = mean_absolute_error(t1, t2)\n",
        "# print(error_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHTDJCqWReot"
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PVN0407RuXv"
      },
      "source": [
        "n_members = 3\n",
        "models = list()\n",
        "\n",
        "# model1 = load_model('absolute-best-model2.h5')\n",
        "model2 = load_model('absolute-best-model3.h5')\n",
        "model3 = load_model('absolute-best-model6.h5')\n",
        "# model4 = load_model('absolute-best-model5.h5')\n",
        "\t# store in memory\n",
        "# models.append(model1)\n",
        "models.append(model2)\n",
        "models.append(model3)\n",
        "# models.append(model4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEwjupKomz2Q"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "img = cv2.imread('img1259.jpg')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "img= cv2.resize(img, dsize=(224,224), interpolation = cv2.INTER_CUBIC)\n",
        "# plt.imshow(img)\n",
        "img.shape\n",
        "\n",
        "img = np.reshape(img, (1, 224, 224))\n",
        "# img = np.expand_dims(img, axis = 0)\n",
        "# img = np.expand_dims(img, axis = 3)\n",
        "# img = np.reshape(128,128)\n",
        "# img = img.astype('float32')\n",
        "img = img/255\n",
        "img = np.repeat(img[..., np.newaxis], 3, -1)\n",
        "# plt.imshow(img.reshape(128,128), cmap = plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPlFIP9VnWcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696a8d09-8037-4839-8fb5-b3f524dd00ea"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcDTSZQUm_b2"
      },
      "source": [
        "img2 = cv2.imread('target_image.png')\n",
        "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "img2= cv2.resize(img2, dsize=(224,224), interpolation = cv2.INTER_CUBIC)\n",
        "# plt.imshow(img)\n",
        "img2.shape\n",
        "\n",
        "img2 = np.reshape(img2, (1, 224, 224))\n",
        "# img = np.expand_dims(img, axis = 0)\n",
        "# img = np.expand_dims(img, axis = 3)\n",
        "# img = np.reshape(128,128)\n",
        "# img = img.astype('float32')\n",
        "img2 = img2/255\n",
        "img2 = np.repeat(img2[..., np.newaxis], 3, -1)\n",
        "# plt.imshow(img.reshape(128,128), cmap = plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e7k8WDWpa1c"
      },
      "source": [
        "img2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGxf25z0lUM8"
      },
      "source": [
        "# predictionimg = model.predict([img,img2])\n",
        "# predictionimg = np.rint(predictionimg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNAbeatNl20z"
      },
      "source": [
        "predictionimg = model2.predict(img2)\n",
        "print(predictionimg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW6Ey-y7Z9Dk",
        "outputId": "ee6b8f7a-2dbc-475d-ce38-e2eb57871360"
      },
      "source": [
        "targetimg = model2.predict(img)\n",
        "print(targetimg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.00883751 -0.00230948 -0.04072975 -0.45861852 -0.37419677]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oSIIfeZ31k9"
      },
      "source": [
        "for i in range(len(predictionimg[:,0])):     #unscaling predictionimg back to original distribution\n",
        "  # print(i)\n",
        "  predictionimg[i,0] = ((predictionimg[i,0] + 1)*(maxb-minb))/2 + minb\n",
        "\n",
        "\n",
        "for i in range(len(predictionimg[:,1])):\n",
        "  # print(i)\n",
        "  predictionimg[i,1] = ((predictionimg[i,1] + 1)*(maxr-minr))/2 + minr   #\n",
        "\n",
        "for i in range(len(predictionimg[:,2])):\n",
        "  # print(i)\n",
        "  predictionimg[i,2] = ((predictionimg[i,2] + 1)*(maxt-mint))/2 + mint \n",
        "\n",
        "for i in range(len(predictionimg[:,3])):\n",
        "  # print(i)\n",
        "  predictionimg[i,3] = ((predictionimg[i,3] + 1)*(maxx-minx))/2 + minx   #\n",
        "\n",
        "for i in range(len(predictionimg[:,4])):\n",
        "  # print(i)\n",
        "  predictionimg[i,4] = ((predictionimg[i,4] + 1)*(maxy-miny))/2 + miny "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h0NOkn7ZaYE"
      },
      "source": [
        "print(predictionimg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOGq9JNlZ20j"
      },
      "source": [
        "for i in range(len(targetimg[:,0])):     #unscaling targetimg back to original distribution\n",
        "  # print(i)\n",
        "  targetimg[i,0] = ((targetimg[i,0] + 1)*(maxb-minb))/2 + minb\n",
        "\n",
        "\n",
        "for i in range(len(targetimg[:,1])):\n",
        "  # print(i)\n",
        "  targetimg[i,1] = ((targetimg[i,1] + 1)*(maxr-minr))/2 + minr   #\n",
        "\n",
        "for i in range(len(targetimg[:,2])):\n",
        "  # print(i)\n",
        "  targetimg[i,2] = ((targetimg[i,2] + 1)*(maxt-mint))/2 + mint \n",
        "\n",
        "for i in range(len(targetimg[:,3])):\n",
        "  # print(i)\n",
        "  targetimg[i,3] = ((targetimg[i,3] + 1)*(maxx-minx))/2 + minx   #\n",
        "\n",
        "for i in range(len(targetimg[:,4])):\n",
        "  # print(i)\n",
        "  targetimg[i,4] = ((targetimg[i,4] + 1)*(maxy-miny))/2 + miny "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGg31zTpaY-4",
        "outputId": "cbb78187-11fc-40c8-cc2c-cc8376c30b5a"
      },
      "source": [
        "print(targetimg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[19.911625   -0.05542746 -0.8145951  16.541382   16.625803  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNg99yJ7aiB7"
      },
      "source": [
        "delta = targetimg - predictionimg\n",
        "print(delta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiJg_pZvlZc0"
      },
      "source": [
        "target =  22  -8 20 16 16.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tMCY8XPl5pH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3idkZYdl5WR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU0bVcavl5Id"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guH2vUqtlZAE"
      },
      "source": [
        "predictionimg2 = model3.predict(img2)\n",
        "print(predictionimg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saJ_EUpwlotO",
        "outputId": "067d9b8c-eee2-4381-a93d-a81d1e7edbf5"
      },
      "source": [
        "targetimg2 = model3.predict(img)\n",
        "print(targetimg2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.2067085  -0.00386199  0.16158421 -0.57697517  0.05799635]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG3YZh_klzmh"
      },
      "source": [
        "for i in range(len(predictionimg2[:,0])):     #unscaling predictionimg2 back to original distribution\n",
        "  # print(i)\n",
        "  predictionimg2[i,0] = ((predictionimg2[i,0] + 1)*(maxb-minb))/2 + minb\n",
        "\n",
        "\n",
        "for i in range(len(predictionimg2[:,1])):\n",
        "  # print(i)\n",
        "  predictionimg2[i,1] = ((predictionimg2[i,1] + 1)*(maxr-minr))/2 + minr   #\n",
        "\n",
        "for i in range(len(predictionimg2[:,2])):\n",
        "  # print(i)\n",
        "  predictionimg2[i,2] = ((predictionimg2[i,2] + 1)*(maxt-mint))/2 + mint \n",
        "\n",
        "for i in range(len(predictionimg2[:,3])):\n",
        "  # print(i)\n",
        "  predictionimg2[i,3] = ((predictionimg2[i,3] + 1)*(maxx-minx))/2 + minx   #\n",
        "\n",
        "for i in range(len(predictionimg2[:,4])):\n",
        "  # print(i)\n",
        "  predictionimg2[i,4] = ((predictionimg2[i,4] + 1)*(maxy-miny))/2 + miny "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO8n7h8tl2UG"
      },
      "source": [
        "print(predictionimg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkCQfXKjl0L9"
      },
      "source": [
        "for i in range(len(targetimg2[:,0])):     #unscaling targetimg2 back to original distribution\n",
        "  # print(i)\n",
        "  targetimg2[i,0] = ((targetimg2[i,0] + 1)*(maxb-minb))/2 + minb\n",
        "\n",
        "\n",
        "for i in range(len(targetimg2[:,1])):\n",
        "  # print(i)\n",
        "  targetimg2[i,1] = ((targetimg2[i,1] + 1)*(maxr-minr))/2 + minr   #\n",
        "\n",
        "for i in range(len(targetimg2[:,2])):\n",
        "  # print(i)\n",
        "  targetimg2[i,2] = ((targetimg2[i,2] + 1)*(maxt-mint))/2 + mint \n",
        "\n",
        "for i in range(len(targetimg2[:,3])):\n",
        "  # print(i)\n",
        "  targetimg2[i,3] = ((targetimg2[i,3] + 1)*(maxx-minx))/2 + minx   #\n",
        "\n",
        "for i in range(len(targetimg2[:,4])):\n",
        "  # print(i)\n",
        "  targetimg2[i,4] = ((targetimg2[i,4] + 1)*(maxy-miny))/2 + miny "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkrue3znmEEU",
        "outputId": "fcdfd0f9-0259-4a06-f8f7-bc3eacee7069"
      },
      "source": [
        "print(targetimg2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[17.932915   -0.09268773  3.2316842  16.423025   17.057997  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwObrsCHmGhV"
      },
      "source": [
        "delta = targetimg2 - predictionimg2\n",
        "print(delta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIYkh1QGmNoV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}