{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "absolute_actuations_exp1_6-0to1-load.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5mcsngV9IjX",
        "outputId": "90e3edaf-6e46-44ca-9bb1-9cdd6272b8b9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aClCGALS9N6c",
        "outputId": "b9b6c7ec-a0a8-4dfa-c9ef-406eb5ba02ec"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive')\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_YleJ_xiBkd",
        "outputId": "b010e5d9-4c16-4dfb-ecee-c16717d7b032"
      },
      "source": [
        "%tensorflow_version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently selected TF version: 2.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUAVH2STVC_U"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "from skimage.io import imread\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras import regularizers\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation, concatenate\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import random\n",
        "import math\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras import callbacks\n",
        "from keras import layers, models\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6iETxCaGyaj"
      },
      "source": [
        "IMG_WIDTH=256\n",
        "IMG_HEIGHT=256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOcam1t5G8sN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09fd72d5-c3bc-4214-da30-0165a2e41b1a"
      },
      "source": [
        "def create_dataset(path):\n",
        "  img_data_array = []\n",
        "\n",
        "  for i in trange(1, 4126):\n",
        "            \n",
        "        path2 = \"img\"+ str(i) +\".jpg\"\n",
        "        path1 = os.path.join(path, path2)\n",
        "        # print(path1)\n",
        "        image= cv2.imread(path1, cv2.COLOR_BGR2RGB)\n",
        "        image=cv2.resize(image, (224, 224),interpolation = cv2.INTER_AREA)\n",
        "        # cv2_imshow(image)\n",
        "        image=np.array(image)\n",
        "        image = image.astype('float32')\n",
        "        image /= 255 \n",
        "        img_data_array.append(image)\n",
        "      \n",
        "        # print(image)\n",
        "\n",
        "  return img_data_array\n",
        "\n",
        "\n",
        "# extract the image array and class name\n",
        "img_data=create_dataset('10psi-images')     #check if the images are mapped correctly to labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/4125 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 11/4125 [00:00<00:39, 102.87it/s]\u001b[A\n",
            "  1%|          | 21/4125 [00:00<00:40, 101.60it/s]\u001b[A\n",
            "  1%|          | 31/4125 [00:00<00:41, 99.21it/s] \u001b[A\n",
            "  1%|          | 40/4125 [00:00<00:43, 93.94it/s]\u001b[A\n",
            "  1%|          | 50/4125 [00:00<00:42, 94.88it/s]\u001b[A\n",
            "  1%|▏         | 60/4125 [00:00<00:42, 96.17it/s]\u001b[A\n",
            "  2%|▏         | 70/4125 [00:00<00:41, 97.01it/s]\u001b[A\n",
            "  2%|▏         | 80/4125 [00:00<00:41, 96.58it/s]\u001b[A\n",
            "  2%|▏         | 90/4125 [00:00<00:43, 93.26it/s]\u001b[A\n",
            "  2%|▏         | 100/4125 [00:01<00:42, 94.63it/s]\u001b[A\n",
            "  3%|▎         | 110/4125 [00:01<00:43, 92.76it/s]\u001b[A\n",
            "  3%|▎         | 120/4125 [00:01<00:43, 92.71it/s]\u001b[A\n",
            "  3%|▎         | 130/4125 [00:01<00:43, 92.46it/s]\u001b[A\n",
            "  3%|▎         | 140/4125 [00:01<00:42, 94.02it/s]\u001b[A\n",
            "  4%|▎         | 150/4125 [00:01<00:41, 94.67it/s]\u001b[A\n",
            "  4%|▍         | 160/4125 [00:01<00:42, 93.60it/s]\u001b[A\n",
            "  4%|▍         | 170/4125 [00:01<00:42, 94.06it/s]\u001b[A\n",
            "  4%|▍         | 180/4125 [00:01<00:42, 93.37it/s]\u001b[A\n",
            "  5%|▍         | 190/4125 [00:02<00:43, 91.33it/s]\u001b[A\n",
            "  5%|▍         | 200/4125 [00:02<00:42, 92.32it/s]\u001b[A\n",
            "  5%|▌         | 210/4125 [00:02<00:41, 93.50it/s]\u001b[A\n",
            "  5%|▌         | 220/4125 [00:02<00:41, 94.50it/s]\u001b[A\n",
            "  6%|▌         | 230/4125 [00:02<00:42, 92.35it/s]\u001b[A\n",
            "  6%|▌         | 240/4125 [00:02<00:41, 93.49it/s]\u001b[A\n",
            "  6%|▌         | 250/4125 [00:02<00:41, 94.18it/s]\u001b[A\n",
            "  6%|▋         | 260/4125 [00:02<00:41, 93.77it/s]\u001b[A\n",
            "  7%|▋         | 270/4125 [00:02<00:40, 94.09it/s]\u001b[A\n",
            "  7%|▋         | 280/4125 [00:02<00:40, 95.18it/s]\u001b[A\n",
            "  7%|▋         | 290/4125 [00:03<00:40, 94.72it/s]\u001b[A\n",
            "  7%|▋         | 301/4125 [00:03<00:39, 96.45it/s]\u001b[A\n",
            "  8%|▊         | 311/4125 [00:03<00:39, 96.12it/s]\u001b[A\n",
            "  8%|▊         | 321/4125 [00:03<00:40, 94.98it/s]\u001b[A\n",
            "  8%|▊         | 331/4125 [00:03<00:39, 95.74it/s]\u001b[A\n",
            "  8%|▊         | 341/4125 [00:03<00:39, 96.68it/s]\u001b[A\n",
            "  9%|▊         | 351/4125 [00:03<00:38, 97.34it/s]\u001b[A\n",
            "  9%|▉         | 361/4125 [00:03<00:38, 97.36it/s]\u001b[A\n",
            "  9%|▉         | 371/4125 [00:03<00:38, 97.55it/s]\u001b[A\n",
            "  9%|▉         | 381/4125 [00:04<00:38, 97.06it/s]\u001b[A\n",
            "  9%|▉         | 391/4125 [00:04<00:40, 93.26it/s]\u001b[A\n",
            " 10%|▉         | 401/4125 [00:04<00:39, 94.27it/s]\u001b[A\n",
            " 10%|▉         | 411/4125 [00:04<00:39, 95.06it/s]\u001b[A\n",
            " 10%|█         | 421/4125 [00:04<00:39, 94.47it/s]\u001b[A\n",
            " 10%|█         | 431/4125 [00:04<00:39, 94.21it/s]\u001b[A\n",
            " 11%|█         | 441/4125 [00:04<00:38, 94.67it/s]\u001b[A\n",
            " 11%|█         | 451/4125 [00:04<00:38, 95.28it/s]\u001b[A\n",
            " 11%|█         | 461/4125 [00:04<00:38, 94.79it/s]\u001b[A\n",
            " 11%|█▏        | 472/4125 [00:04<00:37, 96.53it/s]\u001b[A\n",
            " 12%|█▏        | 482/4125 [00:05<00:38, 95.85it/s]\u001b[A\n",
            " 12%|█▏        | 492/4125 [00:05<00:37, 96.58it/s]\u001b[A\n",
            " 12%|█▏        | 502/4125 [00:05<00:37, 95.66it/s]\u001b[A\n",
            " 12%|█▏        | 512/4125 [00:05<00:40, 89.80it/s]\u001b[A\n",
            " 13%|█▎        | 522/4125 [00:05<00:40, 88.67it/s]\u001b[A\n",
            " 13%|█▎        | 531/4125 [00:05<00:40, 88.31it/s]\u001b[A\n",
            " 13%|█▎        | 541/4125 [00:05<00:40, 89.32it/s]\u001b[A\n",
            " 13%|█▎        | 550/4125 [00:05<00:41, 86.14it/s]\u001b[A\n",
            " 14%|█▎        | 560/4125 [00:05<00:40, 88.47it/s]\u001b[A\n",
            " 14%|█▍        | 570/4125 [00:06<00:39, 89.49it/s]\u001b[A\n",
            " 14%|█▍        | 580/4125 [00:06<00:38, 91.99it/s]\u001b[A\n",
            " 14%|█▍        | 590/4125 [00:06<00:37, 93.84it/s]\u001b[A\n",
            " 15%|█▍        | 601/4125 [00:06<00:36, 96.34it/s]\u001b[A\n",
            " 15%|█▍        | 611/4125 [00:06<00:36, 96.21it/s]\u001b[A\n",
            " 15%|█▌        | 621/4125 [00:06<00:36, 96.61it/s]\u001b[A\n",
            " 15%|█▌        | 631/4125 [00:06<00:36, 96.31it/s]\u001b[A\n",
            " 16%|█▌        | 642/4125 [00:06<00:35, 97.56it/s]\u001b[A\n",
            " 16%|█▌        | 652/4125 [00:06<00:35, 97.07it/s]\u001b[A\n",
            " 16%|█▌        | 662/4125 [00:07<00:36, 95.80it/s]\u001b[A\n",
            " 16%|█▋        | 672/4125 [00:07<00:35, 96.07it/s]\u001b[A\n",
            " 17%|█▋        | 682/4125 [00:07<00:36, 95.06it/s]\u001b[A\n",
            " 17%|█▋        | 693/4125 [00:07<00:35, 96.94it/s]\u001b[A\n",
            " 17%|█▋        | 703/4125 [00:07<00:35, 95.70it/s]\u001b[A\n",
            " 17%|█▋        | 713/4125 [00:07<00:35, 96.75it/s]\u001b[A\n",
            " 18%|█▊        | 723/4125 [00:07<00:35, 96.91it/s]\u001b[A\n",
            " 18%|█▊        | 733/4125 [00:07<00:35, 96.23it/s]\u001b[A\n",
            " 18%|█▊        | 743/4125 [00:07<00:36, 93.72it/s]\u001b[A\n",
            " 18%|█▊        | 753/4125 [00:07<00:36, 93.64it/s]\u001b[A\n",
            " 18%|█▊        | 763/4125 [00:08<00:35, 94.52it/s]\u001b[A\n",
            " 19%|█▊        | 773/4125 [00:08<00:34, 95.78it/s]\u001b[A\n",
            " 19%|█▉        | 783/4125 [00:08<00:34, 95.90it/s]\u001b[A\n",
            " 19%|█▉        | 793/4125 [00:08<00:34, 96.92it/s]\u001b[A\n",
            " 19%|█▉        | 803/4125 [00:08<00:34, 96.62it/s]\u001b[A\n",
            " 20%|█▉        | 813/4125 [00:08<00:35, 94.35it/s]\u001b[A\n",
            " 20%|█▉        | 824/4125 [00:08<00:34, 96.25it/s]\u001b[A\n",
            " 20%|██        | 835/4125 [00:08<00:33, 98.05it/s]\u001b[A\n",
            " 21%|██        | 846/4125 [00:08<00:32, 99.55it/s]\u001b[A\n",
            " 21%|██        | 856/4125 [00:09<00:32, 99.66it/s]\u001b[A\n",
            " 21%|██        | 866/4125 [00:09<00:33, 97.33it/s]\u001b[A\n",
            " 21%|██        | 876/4125 [00:09<00:33, 97.36it/s]\u001b[A\n",
            " 22%|██▏       | 887/4125 [00:09<00:32, 98.65it/s]\u001b[A\n",
            " 22%|██▏       | 898/4125 [00:09<00:32, 99.44it/s]\u001b[A\n",
            " 22%|██▏       | 908/4125 [00:09<00:33, 96.16it/s]\u001b[A\n",
            " 22%|██▏       | 918/4125 [00:09<00:33, 96.15it/s]\u001b[A\n",
            " 23%|██▎       | 929/4125 [00:09<00:32, 97.71it/s]\u001b[A\n",
            " 23%|██▎       | 939/4125 [00:09<00:32, 97.80it/s]\u001b[A\n",
            " 23%|██▎       | 949/4125 [00:09<00:33, 94.69it/s]\u001b[A\n",
            " 33%|███▎      | 1368/4125 [00:29<00:29, 92.45it/s]\n",
            " 23%|██▎       | 969/4125 [00:10<00:33, 93.82it/s]\u001b[A\n",
            " 24%|██▎       | 979/4125 [00:10<00:33, 93.65it/s]\u001b[A\n",
            " 24%|██▍       | 989/4125 [00:10<00:33, 94.88it/s]\u001b[A\n",
            " 24%|██▍       | 999/4125 [00:10<00:33, 93.94it/s]\u001b[A\n",
            " 24%|██▍       | 1009/4125 [00:10<00:32, 95.20it/s]\u001b[A\n",
            " 25%|██▍       | 1019/4125 [00:10<00:32, 96.04it/s]\u001b[A\n",
            " 25%|██▍       | 1029/4125 [00:10<00:32, 94.02it/s]\u001b[A\n",
            " 25%|██▌       | 1039/4125 [00:10<00:32, 95.30it/s]\u001b[A\n",
            " 25%|██▌       | 1049/4125 [00:11<00:32, 95.74it/s]\u001b[A\n",
            " 26%|██▌       | 1059/4125 [00:11<00:32, 94.53it/s]\u001b[A\n",
            " 26%|██▌       | 1069/4125 [00:11<00:32, 95.41it/s]\u001b[A\n",
            " 26%|██▌       | 1079/4125 [00:11<00:31, 96.15it/s]\u001b[A\n",
            " 26%|██▋       | 1089/4125 [00:11<00:31, 95.70it/s]\u001b[A\n",
            " 27%|██▋       | 1099/4125 [00:11<00:32, 92.29it/s]\u001b[A\n",
            " 27%|██▋       | 1110/4125 [00:11<00:31, 95.24it/s]\u001b[A\n",
            " 27%|██▋       | 1120/4125 [00:11<00:31, 95.13it/s]\u001b[A\n",
            " 27%|██▋       | 1130/4125 [00:11<00:31, 95.55it/s]\u001b[A\n",
            " 28%|██▊       | 1140/4125 [00:11<00:31, 96.21it/s]\u001b[A\n",
            " 28%|██▊       | 1151/4125 [00:12<00:30, 97.84it/s]\u001b[A\n",
            " 28%|██▊       | 1162/4125 [00:12<00:29, 98.89it/s]\u001b[A\n",
            " 28%|██▊       | 1173/4125 [00:12<00:29, 100.37it/s]\u001b[A\n",
            " 29%|██▊       | 1184/4125 [00:12<00:29, 99.33it/s] \u001b[A\n",
            " 29%|██▉       | 1194/4125 [00:12<00:29, 99.30it/s]\u001b[A\n",
            " 29%|██▉       | 1204/4125 [00:12<00:29, 98.47it/s]\u001b[A\n",
            " 29%|██▉       | 1214/4125 [00:12<00:29, 97.81it/s]\u001b[A\n",
            " 30%|██▉       | 1224/4125 [00:12<00:29, 97.69it/s]\u001b[A\n",
            " 30%|██▉       | 1234/4125 [00:12<00:29, 97.77it/s]\u001b[A\n",
            " 30%|███       | 1244/4125 [00:13<00:29, 97.53it/s]\u001b[A\n",
            " 30%|███       | 1254/4125 [00:13<00:29, 97.60it/s]\u001b[A\n",
            " 31%|███       | 1264/4125 [00:13<00:29, 95.76it/s]\u001b[A\n",
            " 31%|███       | 1274/4125 [00:13<00:29, 96.14it/s]\u001b[A\n",
            " 31%|███       | 1284/4125 [00:13<00:29, 96.57it/s]\u001b[A\n",
            " 31%|███▏      | 1294/4125 [00:13<00:29, 95.96it/s]\u001b[A\n",
            " 32%|███▏      | 1304/4125 [00:13<00:29, 95.89it/s]\u001b[A\n",
            " 32%|███▏      | 1315/4125 [00:13<00:28, 97.52it/s]\u001b[A\n",
            " 32%|███▏      | 1325/4125 [00:13<00:28, 97.68it/s]\u001b[A\n",
            " 32%|███▏      | 1335/4125 [00:14<00:29, 93.62it/s]\u001b[A\n",
            " 33%|███▎      | 1345/4125 [00:14<00:29, 95.38it/s]\u001b[A\n",
            " 33%|███▎      | 1355/4125 [00:14<00:30, 92.24it/s]\u001b[A\n",
            " 33%|███▎      | 1365/4125 [00:14<00:29, 93.82it/s]\u001b[A\n",
            " 33%|███▎      | 1376/4125 [00:14<00:28, 95.63it/s]\u001b[A\n",
            " 34%|███▎      | 1386/4125 [00:14<00:29, 93.83it/s]\u001b[A\n",
            " 34%|███▍      | 1396/4125 [00:14<00:28, 95.01it/s]\u001b[A\n",
            " 34%|███▍      | 1406/4125 [00:14<00:28, 95.83it/s]\u001b[A\n",
            " 34%|███▍      | 1416/4125 [00:14<00:28, 95.48it/s]\u001b[A\n",
            " 35%|███▍      | 1426/4125 [00:14<00:28, 95.80it/s]\u001b[A\n",
            " 35%|███▍      | 1436/4125 [00:15<00:28, 95.32it/s]\u001b[A\n",
            " 35%|███▌      | 1446/4125 [00:15<00:27, 96.11it/s]\u001b[A\n",
            " 35%|███▌      | 1456/4125 [00:15<00:27, 97.10it/s]\u001b[A\n",
            " 36%|███▌      | 1466/4125 [00:15<00:27, 97.83it/s]\u001b[A\n",
            " 36%|███▌      | 1477/4125 [00:15<00:26, 98.66it/s]\u001b[A\n",
            " 36%|███▌      | 1487/4125 [00:15<00:27, 97.56it/s]\u001b[A\n",
            " 36%|███▋      | 1498/4125 [00:15<00:26, 98.72it/s]\u001b[A\n",
            " 37%|███▋      | 1508/4125 [00:15<00:26, 98.58it/s]\u001b[A\n",
            " 37%|███▋      | 1518/4125 [00:15<00:26, 98.72it/s]\u001b[A\n",
            " 37%|███▋      | 1528/4125 [00:16<00:27, 95.08it/s]\u001b[A\n",
            " 37%|███▋      | 1539/4125 [00:16<00:26, 97.13it/s]\u001b[A\n",
            " 38%|███▊      | 1550/4125 [00:16<00:26, 98.25it/s]\u001b[A\n",
            " 38%|███▊      | 1560/4125 [00:16<00:26, 97.12it/s]\u001b[A\n",
            " 38%|███▊      | 1570/4125 [00:16<00:26, 96.78it/s]\u001b[A\n",
            " 38%|███▊      | 1580/4125 [00:16<00:26, 97.41it/s]\u001b[A\n",
            " 39%|███▊      | 1590/4125 [00:16<00:26, 96.39it/s]\u001b[A\n",
            " 39%|███▉      | 1601/4125 [00:16<00:25, 97.79it/s]\u001b[A\n",
            " 39%|███▉      | 1611/4125 [00:16<00:25, 97.60it/s]\u001b[A\n",
            " 39%|███▉      | 1621/4125 [00:16<00:25, 97.67it/s]\u001b[A\n",
            " 40%|███▉      | 1631/4125 [00:17<00:25, 97.01it/s]\u001b[A\n",
            " 40%|███▉      | 1641/4125 [00:17<00:25, 96.47it/s]\u001b[A\n",
            " 40%|████      | 1651/4125 [00:17<00:25, 97.40it/s]\u001b[A\n",
            " 40%|████      | 1662/4125 [00:17<00:24, 98.70it/s]\u001b[A\n",
            " 41%|████      | 1672/4125 [00:17<00:24, 98.67it/s]\u001b[A\n",
            " 41%|████      | 1682/4125 [00:17<00:25, 97.70it/s]\u001b[A\n",
            " 41%|████      | 1692/4125 [00:17<00:24, 98.27it/s]\u001b[A\n",
            " 41%|████▏     | 1703/4125 [00:17<00:24, 98.15it/s]\u001b[A\n",
            " 42%|████▏     | 1713/4125 [00:17<00:24, 98.22it/s]\u001b[A\n",
            " 42%|████▏     | 1724/4125 [00:18<00:24, 99.62it/s]\u001b[A\n",
            " 42%|████▏     | 1735/4125 [00:18<00:23, 100.70it/s]\u001b[A\n",
            " 42%|████▏     | 1746/4125 [00:18<00:23, 100.21it/s]\u001b[A\n",
            " 43%|████▎     | 1757/4125 [00:18<00:23, 99.64it/s] \u001b[A\n",
            " 43%|████▎     | 1767/4125 [00:18<00:23, 99.30it/s]\u001b[A\n",
            " 43%|████▎     | 1778/4125 [00:18<00:23, 100.33it/s]\u001b[A\n",
            " 43%|████▎     | 1789/4125 [00:18<00:23, 99.16it/s] \u001b[A\n",
            " 44%|████▎     | 1800/4125 [00:18<00:23, 100.18it/s]\u001b[A\n",
            " 44%|████▍     | 1811/4125 [00:18<00:23, 98.78it/s] \u001b[A\n",
            " 44%|████▍     | 1822/4125 [00:18<00:23, 99.57it/s]\u001b[A\n",
            " 44%|████▍     | 1833/4125 [00:19<00:22, 100.27it/s]\u001b[A\n",
            " 45%|████▍     | 1844/4125 [00:19<00:23, 97.99it/s] \u001b[A\n",
            " 45%|████▍     | 1855/4125 [00:19<00:22, 99.55it/s]\u001b[A\n",
            " 45%|████▌     | 1866/4125 [00:19<00:22, 100.95it/s]\u001b[A\n",
            " 46%|████▌     | 1877/4125 [00:19<00:22, 100.01it/s]\u001b[A\n",
            " 46%|████▌     | 1888/4125 [00:19<00:22, 99.09it/s] \u001b[A\n",
            " 46%|████▌     | 1898/4125 [00:19<00:22, 98.54it/s]\u001b[A\n",
            " 46%|████▋     | 1908/4125 [00:19<00:22, 97.60it/s]\u001b[A\n",
            " 46%|████▋     | 1918/4125 [00:19<00:23, 93.93it/s]\u001b[A\n",
            " 47%|████▋     | 1928/4125 [00:20<00:22, 95.67it/s]\u001b[A\n",
            " 47%|████▋     | 1938/4125 [00:20<00:22, 96.74it/s]\u001b[A\n",
            " 47%|████▋     | 1948/4125 [00:20<00:22, 97.64it/s]\u001b[A\n",
            " 47%|████▋     | 1958/4125 [00:20<00:22, 98.05it/s]\u001b[A\n",
            " 48%|████▊     | 1968/4125 [00:20<00:21, 98.12it/s]\u001b[A\n",
            " 48%|████▊     | 1978/4125 [00:20<00:22, 97.42it/s]\u001b[A\n",
            " 48%|████▊     | 1988/4125 [00:20<00:22, 94.55it/s]\u001b[A\n",
            " 48%|████▊     | 1998/4125 [00:20<00:22, 94.39it/s]\u001b[A\n",
            " 49%|████▊     | 2008/4125 [00:20<00:22, 95.34it/s]\u001b[A\n",
            " 49%|████▉     | 2018/4125 [00:21<00:22, 94.25it/s]\u001b[A\n",
            " 49%|████▉     | 2028/4125 [00:21<00:21, 95.51it/s]\u001b[A\n",
            " 49%|████▉     | 2038/4125 [00:21<00:21, 95.12it/s]\u001b[A\n",
            " 50%|████▉     | 2048/4125 [00:21<00:21, 96.03it/s]\u001b[A\n",
            " 50%|████▉     | 2058/4125 [00:21<00:21, 96.76it/s]\u001b[A\n",
            " 50%|█████     | 2069/4125 [00:21<00:20, 98.35it/s]\u001b[A\n",
            " 50%|█████     | 2079/4125 [00:21<00:21, 95.85it/s]\u001b[A\n",
            " 51%|█████     | 2089/4125 [00:21<00:21, 94.40it/s]\u001b[A\n",
            " 51%|█████     | 2099/4125 [00:21<00:21, 95.25it/s]\u001b[A\n",
            " 51%|█████     | 2109/4125 [00:21<00:21, 95.43it/s]\u001b[A\n",
            " 51%|█████▏    | 2119/4125 [00:22<00:21, 93.16it/s]\u001b[A\n",
            " 52%|█████▏    | 2129/4125 [00:22<00:21, 94.17it/s]\u001b[A\n",
            " 52%|█████▏    | 2139/4125 [00:22<00:20, 95.39it/s]\u001b[A\n",
            " 52%|█████▏    | 2150/4125 [00:22<00:20, 97.09it/s]\u001b[A\n",
            " 52%|█████▏    | 2160/4125 [00:22<00:20, 97.82it/s]\u001b[A\n",
            " 53%|█████▎    | 2171/4125 [00:22<00:19, 99.18it/s]\u001b[A\n",
            " 53%|█████▎    | 2181/4125 [00:22<00:19, 97.72it/s]\u001b[A\n",
            " 53%|█████▎    | 2191/4125 [00:22<00:19, 98.14it/s]\u001b[A\n",
            " 53%|█████▎    | 2201/4125 [00:22<00:19, 96.44it/s]\u001b[A\n",
            " 54%|█████▎    | 2211/4125 [00:23<00:19, 96.20it/s]\u001b[A\n",
            " 54%|█████▍    | 2222/4125 [00:23<00:19, 97.87it/s]\u001b[A\n",
            " 54%|█████▍    | 2232/4125 [00:23<00:19, 98.10it/s]\u001b[A\n",
            " 54%|█████▍    | 2243/4125 [00:23<00:19, 98.99it/s]\u001b[A\n",
            " 55%|█████▍    | 2253/4125 [00:23<00:18, 98.98it/s]\u001b[A\n",
            " 55%|█████▍    | 2263/4125 [00:23<00:18, 98.03it/s]\u001b[A\n",
            " 55%|█████▌    | 2273/4125 [00:23<00:19, 97.38it/s]\u001b[A\n",
            " 55%|█████▌    | 2283/4125 [00:23<00:19, 96.18it/s]\u001b[A\n",
            " 56%|█████▌    | 2293/4125 [00:23<00:18, 96.72it/s]\u001b[A\n",
            " 56%|█████▌    | 2303/4125 [00:23<00:18, 96.08it/s]\u001b[A\n",
            " 56%|█████▌    | 2313/4125 [00:24<00:19, 92.37it/s]\u001b[A\n",
            " 56%|█████▋    | 2323/4125 [00:24<00:20, 87.31it/s]\u001b[A\n",
            " 57%|█████▋    | 2333/4125 [00:24<00:20, 89.05it/s]\u001b[A\n",
            " 57%|█████▋    | 2343/4125 [00:24<00:19, 91.99it/s]\u001b[A\n",
            " 57%|█████▋    | 2354/4125 [00:24<00:18, 95.13it/s]\u001b[A\n",
            " 57%|█████▋    | 2364/4125 [00:24<00:18, 96.43it/s]\u001b[A\n",
            " 58%|█████▊    | 2374/4125 [00:24<00:18, 96.04it/s]\u001b[A\n",
            " 58%|█████▊    | 2384/4125 [00:24<00:18, 94.99it/s]\u001b[A\n",
            " 58%|█████▊    | 2394/4125 [00:24<00:18, 93.56it/s]\u001b[A\n",
            " 58%|█████▊    | 2404/4125 [00:25<00:18, 93.17it/s]\u001b[A\n",
            " 59%|█████▊    | 2414/4125 [00:25<00:21, 80.66it/s]\u001b[A\n",
            " 59%|█████▊    | 2423/4125 [00:25<00:21, 78.54it/s]\u001b[A\n",
            " 59%|█████▉    | 2433/4125 [00:25<00:20, 83.17it/s]\u001b[A\n",
            " 59%|█████▉    | 2443/4125 [00:25<00:19, 86.50it/s]\u001b[A\n",
            " 59%|█████▉    | 2453/4125 [00:25<00:18, 89.83it/s]\u001b[A\n",
            " 60%|█████▉    | 2463/4125 [00:25<00:19, 86.85it/s]\u001b[A\n",
            " 60%|█████▉    | 2473/4125 [00:25<00:18, 89.48it/s]\u001b[A\n",
            " 60%|██████    | 2483/4125 [00:25<00:18, 90.63it/s]\u001b[A\n",
            " 60%|██████    | 2493/4125 [00:26<00:17, 90.78it/s]\u001b[A\n",
            " 61%|██████    | 2503/4125 [00:26<00:19, 81.57it/s]\u001b[A\n",
            " 61%|██████    | 2513/4125 [00:26<00:18, 85.75it/s]\u001b[A\n",
            " 61%|██████    | 2524/4125 [00:26<00:17, 90.52it/s]\u001b[A\n",
            " 61%|██████▏   | 2535/4125 [00:26<00:16, 93.56it/s]\u001b[A\n",
            " 62%|██████▏   | 2545/4125 [00:26<00:17, 89.99it/s]\u001b[A\n",
            " 62%|██████▏   | 2555/4125 [00:26<00:17, 90.82it/s]\u001b[A\n",
            " 62%|██████▏   | 2565/4125 [00:26<00:17, 91.56it/s]\u001b[A\n",
            " 62%|██████▏   | 2575/4125 [00:26<00:16, 92.60it/s]\u001b[A\n",
            " 63%|██████▎   | 2585/4125 [00:27<00:16, 94.52it/s]\u001b[A\n",
            " 63%|██████▎   | 2595/4125 [00:27<00:15, 95.65it/s]\u001b[A\n",
            " 63%|██████▎   | 2605/4125 [00:27<00:15, 96.44it/s]\u001b[A\n",
            " 63%|██████▎   | 2616/4125 [00:27<00:15, 97.68it/s]\u001b[A\n",
            " 64%|██████▎   | 2626/4125 [00:27<00:15, 97.28it/s]\u001b[A\n",
            " 64%|██████▍   | 2636/4125 [00:27<00:15, 95.68it/s]\u001b[A\n",
            " 64%|██████▍   | 2646/4125 [00:27<00:15, 94.92it/s]\u001b[A\n",
            " 64%|██████▍   | 2656/4125 [00:27<00:15, 95.88it/s]\u001b[A\n",
            " 65%|██████▍   | 2667/4125 [00:27<00:14, 98.12it/s]\u001b[A\n",
            " 65%|██████▍   | 2677/4125 [00:28<00:14, 97.58it/s]\u001b[A\n",
            " 65%|██████▌   | 2688/4125 [00:28<00:14, 98.52it/s]\u001b[A\n",
            " 65%|██████▌   | 2698/4125 [00:28<00:14, 98.31it/s]\u001b[A\n",
            " 66%|██████▌   | 2708/4125 [00:28<00:14, 96.03it/s]\u001b[A\n",
            " 66%|██████▌   | 2718/4125 [00:28<00:14, 96.31it/s]\u001b[A\n",
            " 66%|██████▌   | 2728/4125 [00:28<00:14, 94.31it/s]\u001b[A\n",
            " 66%|██████▋   | 2739/4125 [00:28<00:14, 96.17it/s]\u001b[A\n",
            " 67%|██████▋   | 2749/4125 [00:28<00:14, 96.07it/s]\u001b[A\n",
            " 67%|██████▋   | 2759/4125 [00:28<00:14, 94.88it/s]\u001b[A\n",
            " 67%|██████▋   | 2769/4125 [00:29<00:14, 94.09it/s]\u001b[A\n",
            " 67%|██████▋   | 2779/4125 [00:29<00:14, 95.36it/s]\u001b[A\n",
            " 68%|██████▊   | 2789/4125 [00:29<00:14, 93.07it/s]\u001b[A\n",
            " 68%|██████▊   | 2799/4125 [00:29<00:14, 94.07it/s]\u001b[A\n",
            " 68%|██████▊   | 2809/4125 [00:29<00:13, 94.88it/s]\u001b[A\n",
            " 68%|██████▊   | 2819/4125 [00:29<00:13, 95.67it/s]\u001b[A\n",
            " 69%|██████▊   | 2830/4125 [00:29<00:13, 97.20it/s]\u001b[A\n",
            " 69%|██████▉   | 2840/4125 [00:29<00:13, 95.08it/s]\u001b[A\n",
            " 69%|██████▉   | 2850/4125 [00:29<00:13, 96.09it/s]\u001b[A\n",
            " 69%|██████▉   | 2860/4125 [00:29<00:13, 95.38it/s]\u001b[A\n",
            " 70%|██████▉   | 2870/4125 [00:30<00:13, 93.84it/s]\u001b[A\n",
            " 70%|██████▉   | 2880/4125 [00:30<00:13, 95.22it/s]\u001b[A\n",
            " 70%|███████   | 2890/4125 [00:30<00:12, 95.56it/s]\u001b[A\n",
            " 70%|███████   | 2900/4125 [00:30<00:12, 96.83it/s]\u001b[A\n",
            " 71%|███████   | 2910/4125 [00:30<00:12, 95.31it/s]\u001b[A\n",
            " 71%|███████   | 2920/4125 [00:30<00:12, 95.39it/s]\u001b[A\n",
            " 71%|███████   | 2931/4125 [00:30<00:12, 97.08it/s]\u001b[A\n",
            " 71%|███████▏  | 2941/4125 [00:30<00:12, 96.76it/s]\u001b[A\n",
            " 72%|███████▏  | 2951/4125 [00:30<00:12, 97.60it/s]\u001b[A\n",
            " 72%|███████▏  | 2961/4125 [00:31<00:12, 96.94it/s]\u001b[A\n",
            " 72%|███████▏  | 2971/4125 [00:31<00:12, 94.29it/s]\u001b[A\n",
            " 72%|███████▏  | 2981/4125 [00:31<00:12, 93.68it/s]\u001b[A\n",
            " 73%|███████▎  | 2991/4125 [00:31<00:11, 94.93it/s]\u001b[A\n",
            " 73%|███████▎  | 3001/4125 [00:31<00:11, 95.93it/s]\u001b[A\n",
            " 73%|███████▎  | 3011/4125 [00:31<00:11, 96.31it/s]\u001b[A\n",
            " 73%|███████▎  | 3021/4125 [00:31<00:11, 96.09it/s]\u001b[A\n",
            " 73%|███████▎  | 3031/4125 [00:31<00:11, 93.92it/s]\u001b[A\n",
            " 74%|███████▎  | 3041/4125 [00:31<00:11, 93.44it/s]\u001b[A\n",
            " 74%|███████▍  | 3051/4125 [00:31<00:11, 93.53it/s]\u001b[A\n",
            " 74%|███████▍  | 3061/4125 [00:32<00:11, 92.00it/s]\u001b[A\n",
            " 74%|███████▍  | 3071/4125 [00:32<00:11, 92.96it/s]\u001b[A\n",
            " 75%|███████▍  | 3081/4125 [00:32<00:11, 94.05it/s]\u001b[A\n",
            " 75%|███████▍  | 3091/4125 [00:32<00:10, 94.81it/s]\u001b[A\n",
            " 75%|███████▌  | 3101/4125 [00:32<00:10, 95.36it/s]\u001b[A\n",
            " 75%|███████▌  | 3112/4125 [00:32<00:10, 97.04it/s]\u001b[A\n",
            " 76%|███████▌  | 3122/4125 [00:32<00:10, 97.81it/s]\u001b[A\n",
            " 76%|███████▌  | 3132/4125 [00:32<00:10, 96.85it/s]\u001b[A\n",
            " 76%|███████▌  | 3142/4125 [00:32<00:10, 97.15it/s]\u001b[A\n",
            " 76%|███████▋  | 3152/4125 [00:33<00:09, 97.34it/s]\u001b[A\n",
            " 77%|███████▋  | 3162/4125 [00:33<00:09, 97.36it/s]\u001b[A\n",
            " 77%|███████▋  | 3172/4125 [00:33<00:10, 92.92it/s]\u001b[A\n",
            " 77%|███████▋  | 3182/4125 [00:33<00:10, 93.84it/s]\u001b[A\n",
            " 77%|███████▋  | 3192/4125 [00:33<00:09, 95.47it/s]\u001b[A\n",
            " 78%|███████▊  | 3202/4125 [00:33<00:09, 95.05it/s]\u001b[A\n",
            " 78%|███████▊  | 3212/4125 [00:33<00:09, 95.26it/s]\u001b[A\n",
            " 78%|███████▊  | 3222/4125 [00:33<00:09, 94.31it/s]\u001b[A\n",
            " 78%|███████▊  | 3232/4125 [00:33<00:09, 92.16it/s]\u001b[A\n",
            " 79%|███████▊  | 3242/4125 [00:33<00:09, 92.62it/s]\u001b[A\n",
            " 79%|███████▉  | 3252/4125 [00:34<00:09, 93.29it/s]\u001b[A\n",
            " 79%|███████▉  | 3262/4125 [00:34<00:09, 94.39it/s]\u001b[A\n",
            " 79%|███████▉  | 3272/4125 [00:34<00:09, 90.09it/s]\u001b[A\n",
            " 80%|███████▉  | 3282/4125 [00:34<00:09, 92.52it/s]\u001b[A\n",
            " 80%|███████▉  | 3292/4125 [00:34<00:08, 94.08it/s]\u001b[A\n",
            " 80%|████████  | 3302/4125 [00:34<00:08, 93.79it/s]\u001b[A\n",
            " 80%|████████  | 3313/4125 [00:34<00:08, 95.76it/s]\u001b[A\n",
            " 81%|████████  | 3323/4125 [00:34<00:08, 95.40it/s]\u001b[A\n",
            " 81%|████████  | 3333/4125 [00:34<00:08, 94.83it/s]\u001b[A\n",
            " 81%|████████  | 3343/4125 [00:35<00:08, 95.09it/s]\u001b[A\n",
            " 81%|████████▏ | 3353/4125 [00:35<00:08, 94.28it/s]\u001b[A\n",
            " 82%|████████▏ | 3364/4125 [00:35<00:07, 96.42it/s]\u001b[A\n",
            " 82%|████████▏ | 3374/4125 [00:35<00:07, 96.80it/s]\u001b[A\n",
            " 82%|████████▏ | 3384/4125 [00:35<00:07, 96.75it/s]\u001b[A\n",
            " 82%|████████▏ | 3395/4125 [00:35<00:07, 98.10it/s]\u001b[A\n",
            " 83%|████████▎ | 3405/4125 [00:35<00:07, 95.32it/s]\u001b[A\n",
            " 83%|████████▎ | 3415/4125 [00:35<00:07, 91.00it/s]\u001b[A\n",
            " 83%|████████▎ | 3425/4125 [00:35<00:07, 91.73it/s]\u001b[A\n",
            " 83%|████████▎ | 3435/4125 [00:36<00:07, 93.28it/s]\u001b[A\n",
            " 84%|████████▎ | 3445/4125 [00:36<00:07, 92.82it/s]\u001b[A\n",
            " 84%|████████▍ | 3455/4125 [00:36<00:07, 86.66it/s]\u001b[A\n",
            " 84%|████████▍ | 3465/4125 [00:36<00:07, 89.77it/s]\u001b[A\n",
            " 84%|████████▍ | 3475/4125 [00:36<00:07, 92.12it/s]\u001b[A\n",
            " 84%|████████▍ | 3485/4125 [00:36<00:06, 93.69it/s]\u001b[A\n",
            " 85%|████████▍ | 3495/4125 [00:36<00:06, 94.32it/s]\u001b[A\n",
            " 85%|████████▍ | 3505/4125 [00:36<00:06, 93.13it/s]\u001b[A\n",
            " 85%|████████▌ | 3515/4125 [00:36<00:06, 93.19it/s]\u001b[A\n",
            " 85%|████████▌ | 3525/4125 [00:36<00:06, 93.44it/s]\u001b[A\n",
            " 86%|████████▌ | 3536/4125 [00:37<00:06, 95.59it/s]\u001b[A\n",
            " 86%|████████▌ | 3546/4125 [00:37<00:06, 95.71it/s]\u001b[A\n",
            " 86%|████████▌ | 3556/4125 [00:37<00:05, 95.40it/s]\u001b[A\n",
            " 86%|████████▋ | 3566/4125 [00:37<00:05, 96.50it/s]\u001b[A\n",
            " 87%|████████▋ | 3576/4125 [00:37<00:05, 96.08it/s]\u001b[A\n",
            " 87%|████████▋ | 3586/4125 [00:37<00:05, 95.09it/s]\u001b[A\n",
            " 87%|████████▋ | 3596/4125 [00:37<00:05, 92.41it/s]\u001b[A\n",
            " 87%|████████▋ | 3606/4125 [00:37<00:05, 89.33it/s]\u001b[A\n",
            " 88%|████████▊ | 3615/4125 [00:37<00:05, 88.37it/s]\u001b[A\n",
            " 88%|████████▊ | 3624/4125 [00:38<00:05, 88.16it/s]\u001b[A\n",
            " 88%|████████▊ | 3634/4125 [00:38<00:05, 89.22it/s]\u001b[A\n",
            " 88%|████████▊ | 3644/4125 [00:38<00:05, 90.50it/s]\u001b[A\n",
            " 89%|████████▊ | 3654/4125 [00:38<00:05, 89.73it/s]\u001b[A\n",
            " 89%|████████▉ | 3664/4125 [00:38<00:05, 90.90it/s]\u001b[A\n",
            " 89%|████████▉ | 3674/4125 [00:38<00:04, 92.74it/s]\u001b[A\n",
            " 89%|████████▉ | 3685/4125 [00:38<00:04, 95.22it/s]\u001b[A\n",
            " 90%|████████▉ | 3695/4125 [00:38<00:04, 95.39it/s]\u001b[A\n",
            " 90%|████████▉ | 3705/4125 [00:38<00:04, 95.35it/s]\u001b[A\n",
            " 90%|█████████ | 3715/4125 [00:39<00:04, 96.48it/s]\u001b[A\n",
            " 90%|█████████ | 3725/4125 [00:39<00:04, 97.10it/s]\u001b[A\n",
            " 91%|█████████ | 3735/4125 [00:39<00:04, 91.85it/s]\u001b[A\n",
            " 91%|█████████ | 3745/4125 [00:39<00:04, 92.91it/s]\u001b[A\n",
            " 91%|█████████ | 3755/4125 [00:39<00:03, 93.24it/s]\u001b[A\n",
            " 91%|█████████▏| 3765/4125 [00:39<00:03, 94.49it/s]\u001b[A\n",
            " 92%|█████████▏| 3775/4125 [00:39<00:03, 95.72it/s]\u001b[A\n",
            " 92%|█████████▏| 3785/4125 [00:39<00:03, 95.84it/s]\u001b[A\n",
            " 92%|█████████▏| 3795/4125 [00:39<00:03, 94.41it/s]\u001b[A\n",
            " 92%|█████████▏| 3805/4125 [00:39<00:03, 94.98it/s]\u001b[A\n",
            " 92%|█████████▏| 3815/4125 [00:40<00:03, 95.10it/s]\u001b[A\n",
            " 93%|█████████▎| 3825/4125 [00:40<00:03, 94.49it/s]\u001b[A\n",
            " 93%|█████████▎| 3835/4125 [00:40<00:03, 94.33it/s]\u001b[A\n",
            " 93%|█████████▎| 3846/4125 [00:40<00:02, 96.71it/s]\u001b[A\n",
            " 93%|█████████▎| 3856/4125 [00:40<00:02, 95.12it/s]\u001b[A\n",
            " 94%|█████████▎| 3866/4125 [00:40<00:02, 95.65it/s]\u001b[A\n",
            " 94%|█████████▍| 3876/4125 [00:40<00:02, 95.90it/s]\u001b[A\n",
            " 94%|█████████▍| 3886/4125 [00:40<00:02, 94.61it/s]\u001b[A\n",
            " 94%|█████████▍| 3896/4125 [00:40<00:02, 94.61it/s]\u001b[A\n",
            " 95%|█████████▍| 3906/4125 [00:41<00:02, 92.28it/s]\u001b[A\n",
            " 95%|█████████▍| 3916/4125 [00:41<00:02, 92.45it/s]\u001b[A\n",
            " 95%|█████████▌| 3926/4125 [00:41<00:02, 93.46it/s]\u001b[A\n",
            " 95%|█████████▌| 3936/4125 [00:41<00:02, 94.36it/s]\u001b[A\n",
            " 96%|█████████▌| 3946/4125 [00:41<00:01, 94.14it/s]\u001b[A\n",
            " 96%|█████████▌| 3956/4125 [00:41<00:01, 94.66it/s]\u001b[A\n",
            " 96%|█████████▌| 3966/4125 [00:41<00:01, 95.17it/s]\u001b[A\n",
            " 96%|█████████▋| 3976/4125 [00:41<00:01, 95.62it/s]\u001b[A\n",
            " 97%|█████████▋| 3986/4125 [00:41<00:01, 93.85it/s]\u001b[A\n",
            " 97%|█████████▋| 3996/4125 [00:42<00:01, 90.40it/s]\u001b[A\n",
            " 97%|█████████▋| 4006/4125 [00:42<00:01, 87.76it/s]\u001b[A\n",
            " 97%|█████████▋| 4016/4125 [00:42<00:01, 89.12it/s]\u001b[A\n",
            " 98%|█████████▊| 4026/4125 [00:42<00:01, 90.38it/s]\u001b[A\n",
            " 98%|█████████▊| 4036/4125 [00:42<00:00, 92.66it/s]\u001b[A\n",
            " 98%|█████████▊| 4046/4125 [00:42<00:00, 90.70it/s]\u001b[A\n",
            " 98%|█████████▊| 4056/4125 [00:42<00:00, 92.04it/s]\u001b[A\n",
            " 99%|█████████▊| 4066/4125 [00:42<00:00, 93.34it/s]\u001b[A\n",
            " 99%|█████████▉| 4076/4125 [00:42<00:00, 91.87it/s]\u001b[A\n",
            " 99%|█████████▉| 4086/4125 [00:43<00:00, 92.68it/s]\u001b[A\n",
            " 99%|█████████▉| 4096/4125 [00:43<00:00, 90.31it/s]\u001b[A\n",
            "100%|█████████▉| 4106/4125 [00:43<00:00, 91.27it/s]\u001b[A\n",
            "100%|██████████| 4125/4125 [00:43<00:00, 95.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VguOMN0XG-2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c0db28-8894-4d17-b5d7-5eeda6bd97a1"
      },
      "source": [
        "train_data = np.asarray(img_data)\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4125, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHcfWA4WIA3z"
      },
      "source": [
        "y_train = pd.read_csv('10psi-labels-all.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy57sGOsBdNh"
      },
      "source": [
        "num_classes = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVMG4LIy51aw",
        "outputId": "4e7b1190-a3f3-417c-9bf4-83d96e3b5b88"
      },
      "source": [
        "train_data.shape\n",
        "# target_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4125, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaXR-fhI1TMc"
      },
      "source": [
        "# train_data = train_data.astype('float32')\n",
        "# train_data = train_data/255\n",
        "\n",
        "# rgb_batch = np.repeat(train_data[..., np.newaxis], 3, -1)\n",
        "# rgb_batch.shape\n",
        "\n",
        "# target_data = target_data.astype('float32')\n",
        "# target_data = target_data/255\n",
        "\n",
        "# rgb_target = np.repeat(target_data[..., np.newaxis], 3, -1)\n",
        "# rgb_target.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8QTLdM0QeBr"
      },
      "source": [
        "# target_data = rgb_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWPUCq7qhC3N"
      },
      "source": [
        "# target_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3J9DPmoEonU"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(train_data, y_train, test_size = 0.3, random_state=600, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcvAQLCmlWL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb2ba6f-1c45-47ab-9792-e232c8718bac"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       B   R  Theta   X   Y\n",
            "2317  18  10    -20  16  18\n",
            "443   22  12    -10  16  16\n",
            "692   20  10     10  16  16\n",
            "2739  30   4     20  16  18\n",
            "2649  22  24     20  16  18\n",
            "...   ..  ..    ...  ..  ..\n",
            "410   20  -4    -10  16  16\n",
            "2506  12 -12     20  16  18\n",
            "3375  16 -24     10  18  16\n",
            "954   20 -16    -20  16  16\n",
            "3275  30 -24    -10  18  16\n",
            "\n",
            "[2887 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEYix5VfIEhe"
      },
      "source": [
        "input_shape = (224, 224, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4n0QR3mlG6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee424470-41f5-48ae-9fa8-565e2f59c0f5"
      },
      "source": [
        "target_scaler = MinMaxScaler()\n",
        "target_scaler.fit(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCloyNullU5f"
      },
      "source": [
        "y_train = target_scaler.transform(y_train)\n",
        "y_test = target_scaler.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtVW342z4PGx"
      },
      "source": [
        "# from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu2TdL3d4Roh"
      },
      "source": [
        "# y_train_trans = StandardScaler().fit_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yKRdRCEagQy"
      },
      "source": [
        "# print(y_train_trans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNj58YWE_6Jp"
      },
      "source": [
        "# print(y_train.iloc[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFnAalhWA-O2"
      },
      "source": [
        "# maxr = y_train.iloc[:,0].max()       #finding the min and max of r, x, y columns in the train data\n",
        "# minr = y_train.iloc[:,0].min()\n",
        "# maxx = y_train.iloc[:,1].max()\n",
        "# minx = y_train.iloc[:,1].min()\n",
        "# maxy = y_train.iloc[:,2].max()\n",
        "# miny = y_train.iloc[:,2].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBHpjTPABEXE"
      },
      "source": [
        "# print(maxr, minr, maxx, minx, maxy, miny)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPzbZ0xgChPG"
      },
      "source": [
        "# y_train_scaled = y_train      #creating another variable y_train_scaled where i copy y_train\n",
        "# y_train_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LzEhPn-DEjc"
      },
      "source": [
        "# len(y_train_scaled.iloc[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fiTHrVbB1ZE"
      },
      "source": [
        "# for i in range(len(y_train_scaled.iloc[:,0])):      #scaling r\n",
        "#   # print(i)\n",
        "#   y_train_scaled.iloc[i,0] = (2 *(y_train_scaled.iloc[i,0] - minr)/(maxr-minr)) - 1   #\n",
        "\n",
        "# for i in range(len(y_train_scaled.iloc[:,1])):    #scaling x\n",
        "#   # print(i)\n",
        "#   y_train_scaled.iloc[i,1] = (2 *(y_train_scaled.iloc[i,1] - minx)/(maxx-minx)) - 1\n",
        "\n",
        "# for i in range(len(y_train_scaled.iloc[:,2])):    #scaling y\n",
        "#   # print(i)\n",
        "#   y_train_scaled.iloc[i,2] = (2 *(y_train_scaled.iloc[i,2] - miny)/(maxy-miny)) - 1   #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ136YtxDoJJ"
      },
      "source": [
        "# y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD1m_MT1D3oC"
      },
      "source": [
        "# maxtr = y_test.iloc[:,0].max()    #finding the min and max of r, x, y columns in the test data\n",
        "# mintr = y_test.iloc[:,0].min()\n",
        "# maxtx = y_test.iloc[:,1].max()\n",
        "# mintx = y_test.iloc[:,1].min()\n",
        "# maxty = y_test.iloc[:,2].max()\n",
        "# minty = y_test.iloc[:,2].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7635z3kZEWoM"
      },
      "source": [
        "# print(maxtr, mintr, maxtx, mintx, maxty, minty)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsLosVosECGp"
      },
      "source": [
        "# y_test_scaled = y_test\n",
        "# y_test_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyV2A8bREN_Q"
      },
      "source": [
        "# for i in range(len(y_test_scaled.iloc[:,0])):     #scaling r\n",
        "#   # print(i)\n",
        "#   y_test_scaled.iloc[i,0] = (2 *(y_test_scaled.iloc[i,0] - mintr)/(maxtr-mintr)) - 1   #\n",
        "\n",
        "# for i in range(len(y_test_scaled.iloc[:,1])):   #scaling x\n",
        "#   # print(i)\n",
        "#   y_test_scaled.iloc[i,1] = (2 *(y_test_scaled.iloc[i,1] - mintx)/(maxtx-mintx)) - 1 \n",
        "\n",
        "# for i in range(len(y_test_scaled.iloc[:,2])):   #scaling y\n",
        "#   # print(i)\n",
        "#   y_test_scaled.iloc[i,2] = (2 *(y_test_scaled.iloc[i,2] - minty)/(maxty-minty)) - 1  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2Psqj0uEoL2"
      },
      "source": [
        "# print(y_test_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kElza0GiF1v7"
      },
      "source": [
        "# print(y_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q34EzcOvrot"
      },
      "source": [
        "# vgg = VGG16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDg0jryivygj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb27816-ea74-4bc8-8682-4d6082d0af8e"
      },
      "source": [
        "new_input = Input(shape=(224, 224, 3))\n",
        "vgg_model = VGG16(include_top=False, weights='imagenet', input_tensor=new_input)\n",
        "\n",
        "for layer in vgg_model.layers[:14]:\n",
        "  layer.trainable = False\n",
        "\n",
        "for layer in vgg_model.layers:\n",
        "    print(layer, layer.trainable)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x7f906038c890> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f9060380350> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f905fa49050> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f9051ab2450> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f9051abd1d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f90603c6dd0> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f9051abd790> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f9050048e10> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f90500503d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f9050055c10> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f90500487d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f905005e690> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f9050066310> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f905005e510> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f905006d7d0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f9050072e90> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f9051abd190> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f905006aed0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f901414f150> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlmvUKDGMzlZ"
      },
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(vgg_model)\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0005)))\n",
        "  model.add(layers.Dropout(0.4))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(5, activation='sigmoid', kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0005)))\n",
        "  model.add(layers.Dropout(0.4))\n",
        "  model.compile(optimizer= Adam(0.0003),#\"adam\",\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mse'])\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKPfZY44uvtM"
      },
      "source": [
        "# def create_model():\n",
        "#   model = Sequential()\n",
        "#   model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))#\n",
        "#   # model.add(layers.MaxPooling2D((2, 2), strides = 2))\n",
        "#   model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "#   model.add(layers.MaxPooling2D((2, 2), strides = 2))\n",
        "#   # model.add(layers.Dropout(0.25))\n",
        "#   model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "#   # model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "#   model.add(layers.MaxPooling2D((2, 2)))\n",
        "#   # model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "#   # model.add(layers.MaxPooling2D((2, 2)))\n",
        "#   # model.add(layers.Dropout(0.25))\n",
        "#   model.add(layers.Flatten())\n",
        "#   model.add(layers.BatchNormalization())\n",
        "#   model.add(layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0005)))\n",
        "#   model.add(layers.Dropout(0.4))\n",
        "  \n",
        "#   model.add(layers.BatchNormalization())\n",
        "  \n",
        "#   model.add(layers.Dense(3, activation='tanh', kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0005)))\n",
        "#   model.add(layers.Dropout(0.4) )  # L1 + L2 penalties\n",
        "#   model.compile(optimizer= Adam(0.0005), # adam\n",
        "#               loss='mean_squared_error',\n",
        "#               metrics=['mse'])\n",
        "  \n",
        "#   return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9G2CdXE6S2y"
      },
      "source": [
        "#  def create_model():  #alexnet\n",
        "\n",
        "#    model = Sequential()\n",
        "#    model.add(layers.Conv2D(96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(224, 224, 3)))\n",
        "#    model.add(layers.BatchNormalization())\n",
        "#    model.add(layers.MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "\n",
        "#    model.add(layers.Conv2D(256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"))\n",
        "#   #  model.add(layers.BatchNormalization())\n",
        "#    model.add(layers.MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "\n",
        "#    model.add(layers.Conv2D(384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
        "#    model.add(layers.BatchNormalization())\n",
        "\n",
        "#    model.add(layers.Conv2D(384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
        "#   #  model.add(layers.BatchNormalization())\n",
        "\n",
        "#    model.add(layers.Conv2D(256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
        "#    model.add(layers.BatchNormalization())\n",
        "#    model.add(layers.MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "\n",
        "#    model.add(layers.Flatten())\n",
        "#   #  model.add(layers.Dense(4096, activation=\"relu\"))\n",
        "#   #  model.add(layers.Dropout(0.5))\n",
        "#    model.add(layers.BatchNormalization())\n",
        "#    model.add(layers.Dense(2048, activation=\"relu\"))\n",
        "#    model.add(layers.Dropout(0.5))\n",
        "#    model.add(layers.Dense(3, activation='tanh'))\n",
        "\n",
        "#    model.compile(optimizer= Adam(0.0005),\n",
        "#               loss='mean_squared_error',\n",
        "#               metrics=['mse'])\n",
        "   \n",
        "#    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGh1O-V4m7Ub",
        "outputId": "e9072109-8fd1-46f5-c06a-c4058262e9d5"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 25088)             100352    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                1605696   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 325       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 16,421,317\n",
            "Trainable params: 8,735,749\n",
            "Non-trainable params: 7,685,568\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rHsTXpiU4Zw"
      },
      "source": [
        "# best_weights_file=\"weights.aug.best.hdf5\"\n",
        "# checkpoint = ModelCheckpoint(best_weights_file, monitor='val_mse', verbose=1, save_best_only=True, mode='max')\n",
        "# callbacks = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sn39rYd0eLt"
      },
      "source": [
        "# reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "#                                patience=5, min_lr=0.0001)   #cyclic scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU7DqMgb-6gU"
      },
      "source": [
        "#print gt and prediction and loss/error for each actuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d9bsakux3NL"
      },
      "source": [
        "# initial_learning_rate = 0.001\n",
        "# def lr_step_decay(epoch, lr):\n",
        "#     drop_rate = 0.5\n",
        "#     epochs_drop = 10.0\n",
        "#     return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipbxNNNkNIo8"
      },
      "source": [
        "initial_learning_rate = 0.01\n",
        "epochs = 100\n",
        "decay = initial_learning_rate / epochs\n",
        "def lr_time_based_decay(epoch, lr):\n",
        "    return lr * 1 / (1 + decay * epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LLcG_DpWKp7",
        "outputId": "6b11695e-9adc-4434-a060-b193e5638896"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "          epochs=150,\n",
        "          batch_size = 64,\n",
        "          validation_split=0.3,\n",
        "          callbacks=[callbacks.LearningRateScheduler(lr_time_based_decay, verbose=1)],\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0003000000142492354.\n",
            "32/32 [==============================] - 56s 900ms/step - loss: 1.6289 - mse: 0.4174 - val_loss: 1.1882 - val_mse: 0.1998\n",
            "Epoch 2/150\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00029997001724751065.\n",
            "32/32 [==============================] - 14s 427ms/step - loss: 1.2890 - mse: 0.3571 - val_loss: 0.9513 - val_mse: 0.1666\n",
            "Epoch 3/150\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00029991002619479554.\n",
            "32/32 [==============================] - 13s 424ms/step - loss: 1.0747 - mse: 0.3312 - val_loss: 0.7854 - val_mse: 0.1620\n",
            "Epoch 4/150\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0002998200791817087.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.8995 - mse: 0.3126 - val_loss: 0.6734 - val_mse: 0.1901\n",
            "Epoch 5/150\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00029970018519144483.\n",
            "32/32 [==============================] - 13s 405ms/step - loss: 0.7623 - mse: 0.3110 - val_loss: 0.5278 - val_mse: 0.1648\n",
            "Epoch 6/150\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00029955041138217907.\n",
            "32/32 [==============================] - 13s 405ms/step - loss: 0.6336 - mse: 0.2946 - val_loss: 0.4311 - val_mse: 0.1586\n",
            "Epoch 7/150\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0002993707957988605.\n",
            "32/32 [==============================] - 13s 407ms/step - loss: 0.5530 - mse: 0.2961 - val_loss: 0.3690 - val_mse: 0.1566\n",
            "Epoch 8/150\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.00029916137647123097.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.5019 - mse: 0.3024 - val_loss: 0.3319 - val_mse: 0.1674\n",
            "Epoch 9/150\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00029892224957496467.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.4400 - mse: 0.2842 - val_loss: 0.2947 - val_mse: 0.1620\n",
            "Epoch 10/150\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0002986534530919789.\n",
            "32/32 [==============================] - 13s 412ms/step - loss: 0.4079 - mse: 0.2800 - val_loss: 0.2740 - val_mse: 0.1583\n",
            "Epoch 11/150\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00029835511221328043.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.3839 - mse: 0.2723 - val_loss: 0.2627 - val_mse: 0.1616\n",
            "Epoch 12/150\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002980272939361519.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.3607 - mse: 0.2631 - val_loss: 0.2431 - val_mse: 0.1528\n",
            "Epoch 13/150\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00029767009430005695.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.3503 - mse: 0.2596 - val_loss: 0.2460 - val_mse: 0.1581\n",
            "Epoch 14/150\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002972836383721378.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.3375 - mse: 0.2520 - val_loss: 0.2543 - val_mse: 0.1747\n",
            "Epoch 15/150\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0002968680221064376.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.3274 - mse: 0.2486 - val_loss: 0.2180 - val_mse: 0.1446\n",
            "Epoch 16/150\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0002964233995391557.\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.3134 - mse: 0.2405 - val_loss: 0.2838 - val_mse: 0.2139\n",
            "Epoch 17/150\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000295949866530298.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.3044 - mse: 0.2347 - val_loss: 0.2280 - val_mse: 0.1591\n",
            "Epoch 18/150\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0002954476060649051.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.3117 - mse: 0.2429 - val_loss: 0.2248 - val_mse: 0.1549\n",
            "Epoch 19/150\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0002949167429518797.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2986 - mse: 0.2295 - val_loss: 0.2152 - val_mse: 0.1502\n",
            "Epoch 20/150\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0002943574600475719.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2921 - mse: 0.2287 - val_loss: 0.2169 - val_mse: 0.1554\n",
            "Epoch 21/150\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00029376991108959606.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2862 - mse: 0.2243 - val_loss: 0.2253 - val_mse: 0.1630\n",
            "Epoch 22/150\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0002931542787970409.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.2893 - mse: 0.2263 - val_loss: 0.2057 - val_mse: 0.1461\n",
            "Epoch 23/150\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00029251074581606803.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.2765 - mse: 0.2181 - val_loss: 0.2253 - val_mse: 0.1695\n",
            "Epoch 24/150\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0002918395237569935.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.2709 - mse: 0.2155 - val_loss: 0.2162 - val_mse: 0.1617\n",
            "Epoch 25/150\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00029114079511154324.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.2707 - mse: 0.2164 - val_loss: 0.2014 - val_mse: 0.1489\n",
            "Epoch 26/150\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00029041477132988095.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2652 - mse: 0.2127 - val_loss: 0.2536 - val_mse: 0.2006\n",
            "Epoch 27/150\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0002896616637778097.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2645 - mse: 0.2124 - val_loss: 0.1990 - val_mse: 0.1495\n",
            "Epoch 28/150\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0002888816837368137.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2601 - mse: 0.2108 - val_loss: 0.2055 - val_mse: 0.1559\n",
            "Epoch 29/150\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.00028807507142666796.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2604 - mse: 0.2104 - val_loss: 0.2156 - val_mse: 0.1647\n",
            "Epoch 30/150\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0002872420669713374.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2483 - mse: 0.1980 - val_loss: 0.1966 - val_mse: 0.1505\n",
            "Epoch 31/150\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00028638291039902457.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2481 - mse: 0.2024 - val_loss: 0.2042 - val_mse: 0.1597\n",
            "Epoch 32/150\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0002854978706561049.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2378 - mse: 0.1937 - val_loss: 0.1860 - val_mse: 0.1430\n",
            "Epoch 33/150\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0002845871875707232.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2473 - mse: 0.2040 - val_loss: 0.1889 - val_mse: 0.1437\n",
            "Epoch 34/150\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0002836511298835123.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2397 - mse: 0.1947 - val_loss: 0.2053 - val_mse: 0.1615\n",
            "Epoch 35/150\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00028268999523318555.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2440 - mse: 0.2003 - val_loss: 0.1951 - val_mse: 0.1532\n",
            "Epoch 36/150\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0002817040231351717.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2403 - mse: 0.1985 - val_loss: 0.1857 - val_mse: 0.1448\n",
            "Epoch 37/150\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00028069354000773416.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.2351 - mse: 0.1944 - val_loss: 0.1824 - val_mse: 0.1430\n",
            "Epoch 38/150\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00027965881414596414.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2324 - mse: 0.1933 - val_loss: 0.2220 - val_mse: 0.1836\n",
            "Epoch 39/150\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00027860014273169554.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2249 - mse: 0.1865 - val_loss: 0.1886 - val_mse: 0.1503\n",
            "Epoch 40/150\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00027751782282835126.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.2313 - mse: 0.1930 - val_loss: 0.1947 - val_mse: 0.1558\n",
            "Epoch 41/150\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0002764121803688813.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2302 - mse: 0.1911 - val_loss: 0.1835 - val_mse: 0.1452\n",
            "Epoch 42/150\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 0.00027528351217140276.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2350 - mse: 0.1969 - val_loss: 0.1943 - val_mse: 0.1573\n",
            "Epoch 43/150\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 0.0002741321439179077.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2304 - mse: 0.1939 - val_loss: 0.2046 - val_mse: 0.1682\n",
            "Epoch 44/150\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 0.000272958430139893.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2216 - mse: 0.1851 - val_loss: 0.1918 - val_mse: 0.1551\n",
            "Epoch 45/150\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 0.00027176266727499496.\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.2196 - mse: 0.1822 - val_loss: 0.2070 - val_mse: 0.1686\n",
            "Epoch 46/150\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0002705452095897059.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2274 - mse: 0.1893 - val_loss: 0.1993 - val_mse: 0.1627\n",
            "Epoch 47/150\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 0.00026930641120946044.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2269 - mse: 0.1905 - val_loss: 0.1748 - val_mse: 0.1394\n",
            "Epoch 48/150\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 0.00026804659715102327.\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.2214 - mse: 0.1862 - val_loss: 0.1749 - val_mse: 0.1400\n",
            "Epoch 49/150\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0002667661212665729.\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.2280 - mse: 0.1933 - val_loss: 0.1846 - val_mse: 0.1499\n",
            "Epoch 50/150\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0002654653372674371.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2244 - mse: 0.1890 - val_loss: 0.1819 - val_mse: 0.1451\n",
            "Epoch 51/150\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0002641446276831983.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2177 - mse: 0.1811 - val_loss: 0.1691 - val_mse: 0.1346\n",
            "Epoch 52/150\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0002628043169788953.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2162 - mse: 0.1824 - val_loss: 0.1645 - val_mse: 0.1313\n",
            "Epoch 53/150\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 0.00026144481635027275.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2263 - mse: 0.1929 - val_loss: 0.1916 - val_mse: 0.1580\n",
            "Epoch 54/150\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 0.00026006644997828384.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2137 - mse: 0.1800 - val_loss: 0.1818 - val_mse: 0.1483\n",
            "Epoch 55/150\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 0.00025866962875744764.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2177 - mse: 0.1843 - val_loss: 0.2146 - val_mse: 0.1812\n",
            "Epoch 56/150\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 0.00025725473447418937.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2156 - mse: 0.1823 - val_loss: 0.1740 - val_mse: 0.1413\n",
            "Epoch 57/150\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 0.00025582211982131347.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2099 - mse: 0.1774 - val_loss: 0.2006 - val_mse: 0.1692\n",
            "Epoch 58/150\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0002543721952291047.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2080 - mse: 0.1767 - val_loss: 0.1776 - val_mse: 0.1467\n",
            "Epoch 59/150\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 0.00025290534202862053.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.2126 - mse: 0.1816 - val_loss: 0.1758 - val_mse: 0.1449\n",
            "Epoch 60/150\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 0.00025142194139928095.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2138 - mse: 0.1829 - val_loss: 0.1765 - val_mse: 0.1462\n",
            "Epoch 61/150\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0002499224032991923.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.2078 - mse: 0.1779 - val_loss: 0.1688 - val_mse: 0.1398\n",
            "Epoch 62/150\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0002484071085960985.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2098 - mse: 0.1809 - val_loss: 0.1666 - val_mse: 0.1377\n",
            "Epoch 63/150\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0002468764669308337.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2023 - mse: 0.1731 - val_loss: 0.2006 - val_mse: 0.1715\n",
            "Epoch 64/150\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0002453308877814015.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1983 - mse: 0.1692 - val_loss: 0.1708 - val_mse: 0.1421\n",
            "Epoch 65/150\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0002437707515443055.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.2024 - mse: 0.1740 - val_loss: 0.1588 - val_mse: 0.1312\n",
            "Epoch 66/150\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0002421964673807505.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1986 - mse: 0.1710 - val_loss: 0.1586 - val_mse: 0.1306\n",
            "Epoch 67/150\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 0.00024060845874585235.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1964 - mse: 0.1681 - val_loss: 0.1530 - val_mse: 0.1248\n",
            "Epoch 68/150\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 0.00023900710556127322.\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.1999 - mse: 0.1721 - val_loss: 0.1624 - val_mse: 0.1351\n",
            "Epoch 69/150\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 0.00023739283095862472.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1953 - mse: 0.1680 - val_loss: 0.1587 - val_mse: 0.1318\n",
            "Epoch 70/150\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 0.00023576604344923048.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.2014 - mse: 0.1742 - val_loss: 0.1722 - val_mse: 0.1442\n",
            "Epoch 71/150\n",
            "\n",
            "Epoch 00071: LearningRateScheduler reducing learning rate to 0.00023412715138214558.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.1981 - mse: 0.1701 - val_loss: 0.1733 - val_mse: 0.1456\n",
            "Epoch 72/150\n",
            "\n",
            "Epoch 00072: LearningRateScheduler reducing learning rate to 0.00023247656294423687.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1966 - mse: 0.1692 - val_loss: 0.1806 - val_mse: 0.1539\n",
            "Epoch 73/150\n",
            "\n",
            "Epoch 00073: LearningRateScheduler reducing learning rate to 0.00023081470060815429.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.1955 - mse: 0.1691 - val_loss: 0.1597 - val_mse: 0.1340\n",
            "Epoch 74/150\n",
            "\n",
            "Epoch 00074: LearningRateScheduler reducing learning rate to 0.00022914195778587094.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1892 - mse: 0.1637 - val_loss: 0.1631 - val_mse: 0.1378\n",
            "Epoch 75/150\n",
            "\n",
            "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0002274587566231954.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1888 - mse: 0.1636 - val_loss: 0.1574 - val_mse: 0.1325\n",
            "Epoch 76/150\n",
            "\n",
            "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0002257655190983348.\n",
            "32/32 [==============================] - 13s 407ms/step - loss: 0.1881 - mse: 0.1633 - val_loss: 0.1601 - val_mse: 0.1356\n",
            "Epoch 77/150\n",
            "\n",
            "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0002240626381376685.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1891 - mse: 0.1645 - val_loss: 0.1563 - val_mse: 0.1320\n",
            "Epoch 78/150\n",
            "\n",
            "Epoch 00078: LearningRateScheduler reducing learning rate to 0.00022235053539304957.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1941 - mse: 0.1699 - val_loss: 0.1695 - val_mse: 0.1457\n",
            "Epoch 79/150\n",
            "\n",
            "Epoch 00079: LearningRateScheduler reducing learning rate to 0.000220629617909687.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1925 - mse: 0.1684 - val_loss: 0.1574 - val_mse: 0.1336\n",
            "Epoch 80/150\n",
            "\n",
            "Epoch 00080: LearningRateScheduler reducing learning rate to 0.00021890030700910397.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1869 - mse: 0.1634 - val_loss: 0.1556 - val_mse: 0.1327\n",
            "Epoch 81/150\n",
            "\n",
            "Epoch 00081: LearningRateScheduler reducing learning rate to 0.0002171630094092088.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1865 - mse: 0.1636 - val_loss: 0.1673 - val_mse: 0.1448\n",
            "Epoch 82/150\n",
            "\n",
            "Epoch 00082: LearningRateScheduler reducing learning rate to 0.00021541811723153797.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1833 - mse: 0.1607 - val_loss: 0.1603 - val_mse: 0.1369\n",
            "Epoch 83/150\n",
            "\n",
            "Epoch 00083: LearningRateScheduler reducing learning rate to 0.0002136660513091752.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1825 - mse: 0.1589 - val_loss: 0.1855 - val_mse: 0.1618\n",
            "Epoch 84/150\n",
            "\n",
            "Epoch 00084: LearningRateScheduler reducing learning rate to 0.00021190721787612798.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1832 - mse: 0.1597 - val_loss: 0.1615 - val_mse: 0.1387\n",
            "Epoch 85/150\n",
            "\n",
            "Epoch 00085: LearningRateScheduler reducing learning rate to 0.00021014202300526384.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1867 - mse: 0.1639 - val_loss: 0.1521 - val_mse: 0.1296\n",
            "Epoch 86/150\n",
            "\n",
            "Epoch 00086: LearningRateScheduler reducing learning rate to 0.00020837087260839015.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1809 - mse: 0.1585 - val_loss: 0.1667 - val_mse: 0.1445\n",
            "Epoch 87/150\n",
            "\n",
            "Epoch 00087: LearningRateScheduler reducing learning rate to 0.00020659415800849815.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1797 - mse: 0.1577 - val_loss: 0.1552 - val_mse: 0.1339\n",
            "Epoch 88/150\n",
            "\n",
            "Epoch 00088: LearningRateScheduler reducing learning rate to 0.0002048122847998055.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1822 - mse: 0.1611 - val_loss: 0.1617 - val_mse: 0.1407\n",
            "Epoch 89/150\n",
            "\n",
            "Epoch 00089: LearningRateScheduler reducing learning rate to 0.00020302565841570728.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1820 - mse: 0.1610 - val_loss: 0.1523 - val_mse: 0.1313\n",
            "Epoch 90/150\n",
            "\n",
            "Epoch 00090: LearningRateScheduler reducing learning rate to 0.00020123466970531028.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1769 - mse: 0.1558 - val_loss: 0.1509 - val_mse: 0.1295\n",
            "Epoch 91/150\n",
            "\n",
            "Epoch 00091: LearningRateScheduler reducing learning rate to 0.00019943970936277608.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1808 - mse: 0.1594 - val_loss: 0.1490 - val_mse: 0.1279\n",
            "Epoch 92/150\n",
            "\n",
            "Epoch 00092: LearningRateScheduler reducing learning rate to 0.00019764116792739787.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1814 - mse: 0.1604 - val_loss: 0.1543 - val_mse: 0.1337\n",
            "Epoch 93/150\n",
            "\n",
            "Epoch 00093: LearningRateScheduler reducing learning rate to 0.00019583945020293543.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1752 - mse: 0.1546 - val_loss: 0.1522 - val_mse: 0.1319\n",
            "Epoch 94/150\n",
            "\n",
            "Epoch 00094: LearningRateScheduler reducing learning rate to 0.00019403493199706.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.1774 - mse: 0.1572 - val_loss: 0.1524 - val_mse: 0.1322\n",
            "Epoch 95/150\n",
            "\n",
            "Epoch 00095: LearningRateScheduler reducing learning rate to 0.0001922279889685197.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1803 - mse: 0.1601 - val_loss: 0.1486 - val_mse: 0.1284\n",
            "Epoch 96/150\n",
            "\n",
            "Epoch 00096: LearningRateScheduler reducing learning rate to 0.00019041901104218626.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1799 - mse: 0.1596 - val_loss: 0.1435 - val_mse: 0.1231\n",
            "Epoch 97/150\n",
            "\n",
            "Epoch 00097: LearningRateScheduler reducing learning rate to 0.00018860837357489925.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.1789 - mse: 0.1586 - val_loss: 0.1502 - val_mse: 0.1305\n",
            "Epoch 98/150\n",
            "\n",
            "Epoch 00098: LearningRateScheduler reducing learning rate to 0.00018679645177479797.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1756 - mse: 0.1559 - val_loss: 0.1754 - val_mse: 0.1555\n",
            "Epoch 99/150\n",
            "\n",
            "Epoch 00099: LearningRateScheduler reducing learning rate to 0.00018498360629070452.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1796 - mse: 0.1595 - val_loss: 0.2343 - val_mse: 0.2140\n",
            "Epoch 100/150\n",
            "\n",
            "Epoch 00100: LearningRateScheduler reducing learning rate to 0.00018317022644712266.\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.1793 - mse: 0.1592 - val_loss: 0.1613 - val_mse: 0.1420\n",
            "Epoch 101/150\n",
            "\n",
            "Epoch 00101: LearningRateScheduler reducing learning rate to 0.00018135665819085764.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1693 - mse: 0.1500 - val_loss: 0.1420 - val_mse: 0.1228\n",
            "Epoch 102/150\n",
            "\n",
            "Epoch 00102: LearningRateScheduler reducing learning rate to 0.0001795432761445407.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1750 - mse: 0.1559 - val_loss: 0.1472 - val_mse: 0.1284\n",
            "Epoch 103/150\n",
            "\n",
            "Epoch 00103: LearningRateScheduler reducing learning rate to 0.00017773042597249815.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1749 - mse: 0.1561 - val_loss: 0.1499 - val_mse: 0.1315\n",
            "Epoch 104/150\n",
            "\n",
            "Epoch 00104: LearningRateScheduler reducing learning rate to 0.00017591846760575883.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1722 - mse: 0.1539 - val_loss: 0.1517 - val_mse: 0.1335\n",
            "Epoch 105/150\n",
            "\n",
            "Epoch 00105: LearningRateScheduler reducing learning rate to 0.00017410774643072802.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1703 - mse: 0.1520 - val_loss: 0.1472 - val_mse: 0.1288\n",
            "Epoch 106/150\n",
            "\n",
            "Epoch 00106: LearningRateScheduler reducing learning rate to 0.00017229860769709198.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1698 - mse: 0.1515 - val_loss: 0.1533 - val_mse: 0.1354\n",
            "Epoch 107/150\n",
            "\n",
            "Epoch 00107: LearningRateScheduler reducing learning rate to 0.0001704913965178856.\n",
            "32/32 [==============================] - 13s 407ms/step - loss: 0.1680 - mse: 0.1502 - val_loss: 0.1424 - val_mse: 0.1249\n",
            "Epoch 108/150\n",
            "\n",
            "Epoch 00108: LearningRateScheduler reducing learning rate to 0.00016868645786956003.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1749 - mse: 0.1574 - val_loss: 0.1510 - val_mse: 0.1333\n",
            "Epoch 109/150\n",
            "\n",
            "Epoch 00109: LearningRateScheduler reducing learning rate to 0.00016688410779918268.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1687 - mse: 0.1511 - val_loss: 0.1465 - val_mse: 0.1290\n",
            "Epoch 110/150\n",
            "\n",
            "Epoch 00110: LearningRateScheduler reducing learning rate to 0.00016508469101878448.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1709 - mse: 0.1536 - val_loss: 0.1530 - val_mse: 0.1360\n",
            "Epoch 111/150\n",
            "\n",
            "Epoch 00111: LearningRateScheduler reducing learning rate to 0.0001632885233168403.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1695 - mse: 0.1525 - val_loss: 0.1531 - val_mse: 0.1363\n",
            "Epoch 112/150\n",
            "\n",
            "Epoch 00112: LearningRateScheduler reducing learning rate to 0.00016149592035689616.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1715 - mse: 0.1547 - val_loss: 0.1748 - val_mse: 0.1579\n",
            "Epoch 113/150\n",
            "\n",
            "Epoch 00113: LearningRateScheduler reducing learning rate to 0.00015970719767763143.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1731 - mse: 0.1561 - val_loss: 0.1702 - val_mse: 0.1531\n",
            "Epoch 114/150\n",
            "\n",
            "Epoch 00114: LearningRateScheduler reducing learning rate to 0.00015792267069292005.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1678 - mse: 0.1506 - val_loss: 0.1542 - val_mse: 0.1368\n",
            "Epoch 115/150\n",
            "\n",
            "Epoch 00115: LearningRateScheduler reducing learning rate to 0.00015614264030399937.\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.1734 - mse: 0.1563 - val_loss: 0.1456 - val_mse: 0.1287\n",
            "Epoch 116/150\n",
            "\n",
            "Epoch 00116: LearningRateScheduler reducing learning rate to 0.00015436742167958544.\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.1660 - mse: 0.1491 - val_loss: 0.1516 - val_mse: 0.1347\n",
            "Epoch 117/150\n",
            "\n",
            "Epoch 00117: LearningRateScheduler reducing learning rate to 0.00015259728670862646.\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.1645 - mse: 0.1477 - val_loss: 0.1495 - val_mse: 0.1330\n",
            "Epoch 118/150\n",
            "\n",
            "Epoch 00118: LearningRateScheduler reducing learning rate to 0.00015083255032345312.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1802 - mse: 0.1639 - val_loss: 0.1486 - val_mse: 0.1326\n",
            "Epoch 119/150\n",
            "\n",
            "Epoch 00119: LearningRateScheduler reducing learning rate to 0.00014907348418527675.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1667 - mse: 0.1508 - val_loss: 0.1518 - val_mse: 0.1363\n",
            "Epoch 120/150\n",
            "\n",
            "Epoch 00120: LearningRateScheduler reducing learning rate to 0.00014732037422870599.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1651 - mse: 0.1495 - val_loss: 0.1489 - val_mse: 0.1332\n",
            "Epoch 121/150\n",
            "\n",
            "Epoch 00121: LearningRateScheduler reducing learning rate to 0.00014557349189596891.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1650 - mse: 0.1493 - val_loss: 0.1464 - val_mse: 0.1307\n",
            "Epoch 122/150\n",
            "\n",
            "Epoch 00122: LearningRateScheduler reducing learning rate to 0.00014383310852201486.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1607 - mse: 0.1450 - val_loss: 0.1473 - val_mse: 0.1316\n",
            "Epoch 123/150\n",
            "\n",
            "Epoch 00123: LearningRateScheduler reducing learning rate to 0.0001420994953345673.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1719 - mse: 0.1563 - val_loss: 0.1419 - val_mse: 0.1265\n",
            "Epoch 124/150\n",
            "\n",
            "Epoch 00124: LearningRateScheduler reducing learning rate to 0.00014037290907907538.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1729 - mse: 0.1576 - val_loss: 0.1481 - val_mse: 0.1329\n",
            "Epoch 125/150\n",
            "\n",
            "Epoch 00125: LearningRateScheduler reducing learning rate to 0.00013865360639954787.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1595 - mse: 0.1444 - val_loss: 0.1476 - val_mse: 0.1325\n",
            "Epoch 126/150\n",
            "\n",
            "Epoch 00126: LearningRateScheduler reducing learning rate to 0.00013694182946634147.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1619 - mse: 0.1469 - val_loss: 0.1455 - val_mse: 0.1307\n",
            "Epoch 127/150\n",
            "\n",
            "Epoch 00127: LearningRateScheduler reducing learning rate to 0.00013523783472499257.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1644 - mse: 0.1497 - val_loss: 0.1458 - val_mse: 0.1312\n",
            "Epoch 128/150\n",
            "\n",
            "Epoch 00128: LearningRateScheduler reducing learning rate to 0.00013354184978089877.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.1730 - mse: 0.1584 - val_loss: 0.1419 - val_mse: 0.1274\n",
            "Epoch 129/150\n",
            "\n",
            "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0001318541165175708.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.1595 - mse: 0.1450 - val_loss: 0.1458 - val_mse: 0.1315\n",
            "Epoch 130/150\n",
            "\n",
            "Epoch 00130: LearningRateScheduler reducing learning rate to 0.00013017486235641202.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1591 - mse: 0.1448 - val_loss: 0.1500 - val_mse: 0.1358\n",
            "Epoch 131/150\n",
            "\n",
            "Epoch 00131: LearningRateScheduler reducing learning rate to 0.00012850430026385652.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1644 - mse: 0.1502 - val_loss: 0.1518 - val_mse: 0.1376\n",
            "Epoch 132/150\n",
            "\n",
            "Epoch 00132: LearningRateScheduler reducing learning rate to 0.00012684265748600347.\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.1634 - mse: 0.1494 - val_loss: 0.1447 - val_mse: 0.1310\n",
            "Epoch 133/150\n",
            "\n",
            "Epoch 00133: LearningRateScheduler reducing learning rate to 0.00012519014681690543.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1592 - mse: 0.1455 - val_loss: 0.1348 - val_mse: 0.1209\n",
            "Epoch 134/150\n",
            "\n",
            "Epoch 00134: LearningRateScheduler reducing learning rate to 0.00012354696660569944.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1619 - mse: 0.1478 - val_loss: 0.1395 - val_mse: 0.1255\n",
            "Epoch 135/150\n",
            "\n",
            "Epoch 00135: LearningRateScheduler reducing learning rate to 0.00012191332948272994.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1617 - mse: 0.1478 - val_loss: 0.1382 - val_mse: 0.1244\n",
            "Epoch 136/150\n",
            "\n",
            "Epoch 00136: LearningRateScheduler reducing learning rate to 0.00012028941927825986.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1631 - mse: 0.1494 - val_loss: 0.1520 - val_mse: 0.1384\n",
            "Epoch 137/150\n",
            "\n",
            "Epoch 00137: LearningRateScheduler reducing learning rate to 0.00011867543410667128.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1539 - mse: 0.1403 - val_loss: 0.1453 - val_mse: 0.1317\n",
            "Epoch 138/150\n",
            "\n",
            "Epoch 00138: LearningRateScheduler reducing learning rate to 0.00011707155047129897.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1602 - mse: 0.1467 - val_loss: 0.1457 - val_mse: 0.1324\n",
            "Epoch 139/150\n",
            "\n",
            "Epoch 00139: LearningRateScheduler reducing learning rate to 0.00011547795198275363.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1624 - mse: 0.1491 - val_loss: 0.1388 - val_mse: 0.1256\n",
            "Epoch 140/150\n",
            "\n",
            "Epoch 00140: LearningRateScheduler reducing learning rate to 0.00011389481500300025.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1646 - mse: 0.1513 - val_loss: 0.1426 - val_mse: 0.1294\n",
            "Epoch 141/150\n",
            "\n",
            "Epoch 00141: LearningRateScheduler reducing learning rate to 0.0001123223014734318.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1693 - mse: 0.1562 - val_loss: 0.1336 - val_mse: 0.1205\n",
            "Epoch 142/150\n",
            "\n",
            "Epoch 00142: LearningRateScheduler reducing learning rate to 0.00011076058044635842.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1622 - mse: 0.1492 - val_loss: 0.1454 - val_mse: 0.1322\n",
            "Epoch 143/150\n",
            "\n",
            "Epoch 00143: LearningRateScheduler reducing learning rate to 0.00010920979938515925.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1651 - mse: 0.1519 - val_loss: 0.1423 - val_mse: 0.1292\n",
            "Epoch 144/150\n",
            "\n",
            "Epoch 00144: LearningRateScheduler reducing learning rate to 0.00010767012004181632.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1640 - mse: 0.1509 - val_loss: 0.1486 - val_mse: 0.1357\n",
            "Epoch 145/150\n",
            "\n",
            "Epoch 00145: LearningRateScheduler reducing learning rate to 0.00010614168258651589.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1641 - mse: 0.1512 - val_loss: 0.1357 - val_mse: 0.1229\n",
            "Epoch 146/150\n",
            "\n",
            "Epoch 00146: LearningRateScheduler reducing learning rate to 0.00010462462713417748.\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.1644 - mse: 0.1517 - val_loss: 0.1371 - val_mse: 0.1246\n",
            "Epoch 147/150\n",
            "\n",
            "Epoch 00147: LearningRateScheduler reducing learning rate to 0.00010311908657322389.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1611 - mse: 0.1486 - val_loss: 0.1374 - val_mse: 0.1251\n",
            "Epoch 148/150\n",
            "\n",
            "Epoch 00148: LearningRateScheduler reducing learning rate to 0.00010162519373969265.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1574 - mse: 0.1451 - val_loss: 0.1390 - val_mse: 0.1269\n",
            "Epoch 149/150\n",
            "\n",
            "Epoch 00149: LearningRateScheduler reducing learning rate to 0.00010014307424741782.\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.1552 - mse: 0.1431 - val_loss: 0.1459 - val_mse: 0.1341\n",
            "Epoch 150/150\n",
            "\n",
            "Epoch 00150: LearningRateScheduler reducing learning rate to 9.867284649158816e-05.\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.1560 - mse: 0.1441 - val_loss: 0.1398 - val_mse: 0.1280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc_R1cBMXEOM"
      },
      "source": [
        "#  model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "1pVvmnq6_WX7",
        "outputId": "e5004277-821f-46c2-962e-c0d6b1c8ffc2"
      },
      "source": [
        "plt.plot(history.history['mse'])\n",
        "plt.plot(history.history['val_mse'])\n",
        "plt.title('Model MSE')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Pp8UsinhpG"
      },
      "source": [
        "prediction = model.predict(x_test)\n",
        "prediction = target_scaler.inverse_transform(prediction)\n",
        "prediction = np.rint(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ai-hdEUrMcK",
        "outputId": "41dc845a-1bb8-4cc2-ba09-33b430ee97f2"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 15.   3.   3.  17.  16.]\n",
            " [ 17.  -7. -17.  16.  16.]\n",
            " [ 15.  -8. -17.  16.  17.]\n",
            " ...\n",
            " [ 14.  -2.  -9.  16.  16.]\n",
            " [ 21.  -8. -16.  16.  16.]\n",
            " [ 16.  -7.  -6.  16.  17.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM9oEpO0RSVZ"
      },
      "source": [
        "# for i in range(len(y_test_scaled.iloc[:,0])):       #unscaling y_train back to original distributing\n",
        "#   # print(i)\n",
        "#   y_test_scaled.iloc[i,0] = ((y_test_scaled.iloc[i,0] + 1)*(maxtr-mintr))/2 + mintr\n",
        "\n",
        "\n",
        "# for i in range(len(y_test_scaled.iloc[:,1])):\n",
        "#   # print(i)\n",
        "#   y_test_scaled.iloc[i,1] = ((y_test_scaled.iloc[i,1] + 1)*(maxtx-mintx))/2 + mintx   #\n",
        "\n",
        "# for i in range(len(y_test_scaled.iloc[:,2])):\n",
        "#   # print(i)\n",
        "#   y_test_scaled.iloc[i,2] = ((y_test_scaled.iloc[i,2] + 1)*(maxty-minty))/2 + minty   #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciu3jV4hR5C-"
      },
      "source": [
        "# y_test_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWUwm9HmSbBu"
      },
      "source": [
        "# prediction = model.predict(x_test)    #finding predictions on x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_2Cc8UiYr2y"
      },
      "source": [
        "# prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUQl7xuXSg53"
      },
      "source": [
        "# for i in range(len(prediction[:,0])):     #unscaling prediction back to original distribution\n",
        "#   # print(i)\n",
        "#   prediction[i,0] = ((prediction[i,0] + 1)*(maxtr-mintr))/2 + mintr\n",
        "\n",
        "\n",
        "# for i in range(len(prediction[:,1])):\n",
        "#   # print(i)\n",
        "#   prediction[i,1] = ((prediction[i,1] + 1)*(maxtx-mintx))/2 + mintx   #\n",
        "\n",
        "# for i in range(len(prediction[:,2])):\n",
        "#   # print(i)\n",
        "#   prediction[i,2] = ((prediction[i,2] + 1)*(maxty-minty))/2 + minty "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9settPVSmmo"
      },
      "source": [
        "# prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi7rCA1ATByn"
      },
      "source": [
        "# prediction_round = np.rint(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgEcPPOvTIJN"
      },
      "source": [
        "# prediction_round"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tp_R8uZLMml"
      },
      "source": [
        "# y_test_inv = target_scaler.inverse_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP0uoETYLPsX"
      },
      "source": [
        "# prediction = model.predict(x_test)\n",
        "# prediction = target_scaler.inverse_transform(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnB_UDyJLSyk"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgdKp4DRLVb-"
      },
      "source": [
        "mse = mean_squared_error(y_test, prediction)\n",
        "mae = mean_absolute_error(y_test, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5Cbf9ALYUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0759c1-8361-4f4a-ec44-4098e33979bc"
      },
      "source": [
        "print(mse)\n",
        "print(mae)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "195.05251186950272\n",
            "13.00851373182553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udXes8FKM4eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e71b82d-6941-49a6-8b35-ed2ef464c2ff"
      },
      "source": [
        "model.save(\"0to1-model2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: 0to1-model2/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7N2czQz_QwR"
      },
      "source": [
        "model.save(\"0to1-model2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sFIBcvHkjcA"
      },
      "source": [
        "prediction_round = np.rint(prediction)\n",
        "print(prediction_round)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XkC4i0v3pKH"
      },
      "source": [
        "np.savetxt('alexnet--1to1-rxy.csv', prediction_round, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9HuIRJLkoCT"
      },
      "source": [
        "y_test_inv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecdzdeZd3sgC"
      },
      "source": [
        "np.savetxt('alexnet-truth--1to1-rxy.csv', y_test_scaled, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXmWSHqtk2Nz"
      },
      "source": [
        "# b1 = y_test_inv[:,0]\n",
        "# b2 = prediction_round[:,0]\n",
        "# error_b = mean_absolute_error(b1, b2)\n",
        "# print(error_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kEyoMZRmksi"
      },
      "source": [
        "# r1 = y_test_scaled.iloc[:,0]\n",
        "# r2 = prediction_round[:,0]\n",
        "# error_r = mean_absolute_error(r1, r2)\n",
        "# print(error_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbTZbN0lmsWL"
      },
      "source": [
        "# t1 = y_test_inv[:,2]\n",
        "# t2 = prediction_round[:,2]\n",
        "# error_t = mean_absolute_error(t1, t2)\n",
        "# print(error_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XicKaastjOiK"
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6dxMwCTjU9n"
      },
      "source": [
        "reconstructed_model = load_model('0to1-model2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEwjupKomz2Q"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "img = cv2.imread('img1259.jpg')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "img= cv2.resize(img, dsize=(224,224), interpolation = cv2.INTER_CUBIC)\n",
        "# plt.imshow(img)\n",
        "img.shape\n",
        "\n",
        "img = np.reshape(img, (1, 224, 224))\n",
        "# img = np.expand_dims(img, axis = 0)\n",
        "# img = np.expand_dims(img, axis = 3)\n",
        "# img = np.reshape(128,128)\n",
        "# img = img.astype('float32')\n",
        "img = img/255\n",
        "img = np.repeat(img[..., np.newaxis], 3, -1)\n",
        "# plt.imshow(img.reshape(128,128), cmap = plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPlFIP9VnWcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f37346-feee-4d45-a3b7-7b1d1133cdaf"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcDTSZQUm_b2"
      },
      "source": [
        "img2 = cv2.imread('trial2_7.jpg')\n",
        "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "img2= cv2.resize(img2, dsize=(224,224), interpolation = cv2.INTER_CUBIC)\n",
        "# plt.imshow(img)\n",
        "img2.shape\n",
        "\n",
        "img2 = np.reshape(img2, (1, 224, 224))\n",
        "# img = np.expand_dims(img, axis = 0)\n",
        "# img = np.expand_dims(img, axis = 3)\n",
        "# img = np.reshape(128,128)\n",
        "# img = img.astype('float32')\n",
        "img2 = img2/255\n",
        "img2 = np.repeat(img2[..., np.newaxis], 3, -1)\n",
        "# plt.imshow(img.reshape(128,128), cmap = plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e7k8WDWpa1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66e23fb-f366-4612-af6c-05f8b663a55c"
      },
      "source": [
        "img2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oSIIfeZ31k9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33418aec-84cc-44c8-d097-83911f8c5ae8"
      },
      "source": [
        "predictionimg = reconstructed_model.predict(img2)\n",
        "print(predictionimg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33701262 0.2790821  0.4132681  0.06850087 0.53418946]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crCCX6zobNhp"
      },
      "source": [
        "predictionimg = target_scaler.inverse_transform(predictionimg)\n",
        "predictionimg = np.rint(predictionimg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBCReRWtonwG",
        "outputId": "16dd0268-db6f-4389-992f-cf445c8d528f"
      },
      "source": [
        "print(predictionimg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 17. -11.  -3.  16.  17.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joOG2RiCbQUS",
        "outputId": "2d80cc08-472e-49a7-f828-a909327f9ca5"
      },
      "source": [
        "targetimg = reconstructed_model.predict(img)\n",
        "print(targetimg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.31561142 0.36477828 0.31439748 0.0121838  0.14697085]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMLi6TDTbVaZ"
      },
      "source": [
        "targetimg = target_scaler.inverse_transform(targetimg)\n",
        "targetimg = np.rint(targetimg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZnX8FHXkgbR",
        "outputId": "6ddaaea0-66e2-4bdd-e45b-dc796a92b10b"
      },
      "source": [
        "targetimg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16., -6., -7., 16., 16.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIfwIjeKqjxG"
      },
      "source": [
        "targetac = [22, -8, 20, 16, 16]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHTY9fpWbbsc"
      },
      "source": [
        "delta = targetimg - predictionimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHQvPu2LbsOP",
        "outputId": "7f1188a8-017e-499d-a417-fc81ea4d64c4"
      },
      "source": [
        "print(delta)    # "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-3.  5. -4.  0. -1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjZducWVk6wu"
      },
      "source": [
        "15 0 0 16 16      17 -1 1 16 16     19 -3 2 16 16     21 -6 7 16 16   23 -7 16 16 16   22 -9 17 16 16   22 -8 20 16 16  23 -10 19 16 16   21 -7 21 16 17    22 -8 20 16 17"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBhLFhnouiZB"
      },
      "source": [
        "20 24 18 19 15      21 17 21 19 16      22  9  20 18 16     21 4 21  18 16     23 0 19 18 16    24  -5 22 18 16   22 -7 25 16 16    21 -9  19 16 16   22 -7 20 16 16    21 -9 21 16 16    22 -8 20 16 16    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BhJ9KUgk7gr"
      },
      "source": [
        "target =  22  -8 20 16 16.   "
      ]
    }
  ]
}